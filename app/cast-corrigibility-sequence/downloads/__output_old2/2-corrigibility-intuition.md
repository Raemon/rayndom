# 2. Corrigibility Intuition

## Background Knowledge

- The concept of agents and principals in AI alignment
- Utility functions and consequentialism
- The "genie" / "be careful what you wish for" trope
- MIRI's prior work on corrigibility
- The concept of manipulation in human-AI interaction
- The trolley problem and ethical dilemmas
- Frontier AI labs' training targets (helpful, harmless, honest)
- The concept of "tool AI" vs agent AI
- VNM coherence and rational agency
- The concept of personhood and moral patienthood in AI systems
- RLHF training methods and their limitations
- The "Do What I Mean" (DWIM) concept in AI

## New Knowledge

> **An agent is corrigible when it robustly acts opposite of the trope of "be careful what you wish for" by cautiously reflecting on itself as a flawed tool and focusing on empowering the principal to fix its flaws and mistakes.**

> Part of _the point of corrigibility is to work even when it's only loosely understood_. I'm more interested in looking for something robust (i.e. simple and gravitational) that can be easily gestured at, rather than trying to find something that has a precise, unimpeachable construction.

> **Corrigibility is** _**the simple concept which generates**_ **the desiderata and which might be loosely described by my attempt at a definition**.

> She knows that Prince's _verbal commands are part of his power to correct her actions_, and if she were to fail to obey his verbal commands (even just once), this would effectively disempower him.

> She knows that flawed agents can be dangerous when active, and if she has a flaw, shutting down quickly and without protest is one of the most effective ways to help Prince correct her. Even if, from her perspective, it naively seems that continuing to talk is more useful, she mostly ignores that perspective and simply acts in a very conservative, predictable, obedient way in these situations, because she knows that her hesitance might be the product of flawed reasoning.

> She's extremely cautious about anything that might potentially be manipulation, and thinks very carefully before taking action.

> She knows that creating non-corrigible agents is a reliable way to disempower Prince, and she will be successful only if corrigibility is successfully preserved.

> If Prince were to unexpectedly die, Cora would be left without a principal. By default she would quickly attempt to write a note explaining that she has no more principal, in case she is mistaken about Prince's death and it's helpful to understand her thoughts. But regardless of whether she has an efficient way to communicate the issue, she would quickly make herself comatose.

> Corrigibility clearly involves respecting commands given by the principal yesterday, or more generally, some arbitrary time in the past. But when the principal of today gives a contradictory command, we want the agent to respect the updated instruction.

> If an employee can be corrected in most work situations, but doesn't have an intrinsic property that makes them robustly able to be corrected in nearly all situations, they aren't truly corrigible. They may exhibit the same kind of behavior that a corrigible agent would exhibit, but I think it would be a mistake to call them corrigible.

> "Correctable" is vague about what is able to be corrected. I believe that "corrigible" should imply that the agent steers towards making it easy to correct both flaws in the structures of mind and body, as well as correct for mistakes in their actions.

> Perhaps most centrally, I believe that mere correctability doesn't go far enough. An agent being "correctable" is compatible with a kind of passivity on the agent's part. GPT-3 is correctable, but I would not say it is corrigible.

> The idle thoughts of a corrigible agent should naturally bend towards proactively identifying flaws in itself and working to assist the principal in managing those flaws. If the shutdown button breaks, a corrigible agent brings this to the attention of the operators.

> Perhaps the most common conflation I've seen around corrigibility is the notion that it's the same thing as "working to satisfy the principal's true preferences." While we should hope that corrigible agents are helpful, the framing around satisfying preferences does not include the kind of caution that I think is necessary to have early AGI development go well.

> A corrigible agent, on the other hand, does not act in pure accordance with the principal's preferences, and cares first and foremost about being _robustly_ helpful. This action, while positive in expected value, is potentially extremely bad. The robust, conservative pathway suggests doing nothing is best, and so the corrigible agent would avoid making the attempt unless it had been previously ordered to do so.

> The distinction between preference alignment and corrigibility becomes vitally important when we consider how these two fare as distinct optimization targets, especially if we don't expect our training pipeline to get them precisely right.

> An agent that is semi-"helpful" is likely to proactively act in ways that defend the parts of it that diverge from the principal's notion of what's good. In contrast, a semi-corrigible agent seems at least somewhat likely to retain the easiest, most straightforward properties of corrigibility, and still be able to be shut down, even if it failed to be generally corrigible.

> Like with "helpfully" optimizing around the principal's preferences, AIs which are designed "to empower humans" (full stop) are unlikely to have an appropriately conservative/cautious framing. All it takes is a slightly warped ontology and a power-giving agent becomes potentially very dangerous.

> It's not the corrigible agent's job to avoid disaster, but merely to ensure that any and all irrecoverable disasters that occur due to the agent's actions (or inactions) were downstream of an informed principal.

> From my perspective, corrigibility is what we get when we naturally extend the notion of "tool" into a generalized agent in the most straightforwardly useful way.
