# 0. CAST: Corrigibility as Singular Target

## Background Knowledge

- AI alignment as a field of research
- Prosaic training methods (RLHF, etc.)
- The concept of "acute risk period" in AI development
- CEV (Coherent Extrapolated Volition) as a prior alignment proposal
- Frontier AI labs (OpenAI, DeepMind, Anthropic) and their current approaches
- The concept of reinforcement learning and gradient descent
- The Omohundro Drives / instrumentally convergent subgoals
- The concept of a "shutdown button" and related AI safety challenges
- The difference between outer alignment and inner alignment
- The notion of "alignment tax" in AI development
- Paul Christiano's work on corrigibility and AI alignment
- The concept of distributional shift in machine learning

## New Knowledge

> I now think that **corrigibility is a single, intuitive property**, which people can learn to emulate without too much work and which is deeply compatible with agency.

> Furthermore, I expect that even with prosaic training methods, there's some chance of winding up with an AI agent that's inclined to become more corrigible over time, rather than less (as long as the people who built it understand corrigibility and want that agent to become more corrigible).

> Through a slow, gradual, and careful process of refinement, I see a path forward where **this sort of agent could ultimately wind up as a (mostly) safe superintelligence**.

> **This is not the path we are currently on.** As far as I can tell, frontier labs do not understand corrigibility deeply, and are not training their models with corrigibility as the goal.

> **Building corrigible agents is hard and fraught with challenges.** Even in an ideal world where the developers of AGI aren't racing ahead, but are free to go as slowly as they wish and take all the precautions I indicate, there are good reasons to think doom is still likely.

> **Corrigibility is the simple, underlying generator** behind obedience, conservatism, willingness to be shut down and modified, transparency, and low-impact.

> **Aiming for CAST is a better plan than aiming for human values** (i.e. CEV), helpfulness+harmlessness+honesty, or even a balanced collection of desiderata, including some of the things corrigibility gives rise to.

> **CAST is a target, rather than a technique**, and as such it's compatible both with prosaic methods and superior architectures.

> Contra Paul Christiano, we should not expect corrigibility to emerge automatically from systems trained to satisfy local human preferences.

> **Corrigibility is nearly unique among all goals for being simultaneously useful and non-self-protective.**

> This property of non-self-protection means we should suspect AIs that are almost-corrigible will assist, rather than resist, being made more corrigible, thus forming **an attractor-basin around corrigibility**, such that almost-corrigible systems gradually become truly corrigible by being modified by their creators.

> There is also reason to suspect that almost-corrigible AIs learn to be less corrigible over time due to **corrigibility being "anti-natural."** It is unclear to me which of these forces will win out in practice.

> Most notably, **corrigible AI is still extremely vulnerable to misuse**, and we must ensure that superintelligent AGI is only ever corrigible to wise representatives.
