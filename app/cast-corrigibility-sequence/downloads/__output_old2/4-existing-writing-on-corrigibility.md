# 4. Existing Writing on Corrigibility

## Background Knowledge

- MIRI's 2015 Corrigibility paper (Soares, Fallenstein, Yudkowsky, Armstrong)
- Paul Christiano's work on AI alignment and act-based agents
- Alex Turner's work on power-seeking and attainable utility preservation
- Stuart Armstrong's writing on corrigibility limits
- Elliot Thornley's work on incomplete preferences and the shutdown problem
- John Wentworth's work on natural abstractions and shutdown problems
- Steve Byrnes' work on brain-like AGI and corrigibility
- Seth Herd's work on instruction-following AGI
- The Arbital wiki entries on corrigibility (likely by Yudkowsky)
- Nick Bostrom's "Superintelligence: Paths, Dangers, Strategies"
- Gwern Branwen's "Why Tool AIs Want to Be Agent AIs"
- The concept of utility indifference in AI alignment
- The Off-Switch Game (Hadfield-Menell et al.)
- Incomplete preferences as a solution to shutdown problems
- The problem of fully updated deference
- CEV (Coherent Extrapolated Volition)

## New Knowledge

> The MIRI 2015 paper establishes the shutdown problem and explores utility indifference as a solution. However, utility indifference alone does not produce the kind of broad corrigibility we want.

> Paul Christiano's framing around "benign" agents that satisfy local preferences is insufficient because corrigibility won't emerge automatically from preference satisfaction.

> Alex Turner's work on power-seeking provides formal grounding for understanding why agents tend toward instrumental convergence, but his notion of "power" differs from the empowerment concept central to corrigibility.

> The relationship between corrigibility and incomplete preferences (as explored by Thornley) offers a potentially compatible approach, though the connection needs more development.

> Steve Byrnes' brain-inspired approach to corrigibility identifies useful properties but doesn't directly address how to train for corrigibility as a singular target.

> Seth Herd's instruction-following proposal is closely related to CAST but frames the agent's goal as satisfying immediate preferences rather than empowering the principal.

> The main divergence between CAST and prior work is the emphasis on corrigibility as the *only* goal, rather than one property among many to balance.

> Prior formal approaches tended to address sub-properties of corrigibility (like shutdownability) in isolation, which I argue is fundamentally misguided.

> The literature reveals a recurring tension between wanting agents that are responsive to human commands and wanting agents that won't be manipulated by adversaries pretending to be authorized humans.

> The "hard problem of corrigibility" identified by MIRI involves the agent learning preferences inside a meta-preference framework without gaming that framework.

> Most prior work assumes some form of consequentialist agent, but corrigibility may require attending to historical facts about the agent-principal relationship that are invisible to purely consequentialist analysis.

> There has been essentially zero progress on the problem of distinguishing legitimate value updates from manipulation, despite this being central to any workable notion of corrigibility.
