# 0. CAST: Corrigibility as Singular Target - Concept Analysis

## Background Knowledge

- The concept of corrigibility in AI alignment literature
- CEV (Coherent Extrapolated Volition)
- The acute risk period in AI development
- RLHF (Reinforcement Learning from Human Feedback)
- The concept of value fragility
- Omohundro Drives / instrumental convergence
- Anti-naturality of corrigibility (Yudkowsky)
- Prosaic vs non-prosaic AI alignment methods
- The shutdown problem
- Paul Christiano's work on corrigibility and preference satisfaction
- Alex Turner's work on power and AUP (Attainable Utility Preservation)

## New Knowledge

### Core Thesis

> I now think that corrigibility is a single, intuitive property, which people can learn to emulate without too much work and which is deeply compatible with agency.

> Through a slow, gradual, and careful process of refinement, I see a path forward where this sort of agent could ultimately wind up as a (mostly) safe superintelligence.

### Key Claims

> Corrigibility is the simple, underlying generator behind obedience, conservatism, willingness to be shut down and modified, transparency, and low-impact.

> Aiming for CAST is a better plan than aiming for human values (i.e. CEV), helpfulness+harmlessness+honesty, or even a balanced collection of desiderata, including some of the things corrigibility gives rise to.

> CAST is a target, rather than a technique, and as such it's compatible both with prosaic methods and superior architectures.

> Contra Paul Christiano, we should not expect corrigibility to emerge automatically from systems trained to satisfy local human preferences.

### The Attractor Basin Claim

> Corrigibility is nearly unique among all goals for being simultaneously useful and non-self-protective.

> This property of non-self-protection means we should suspect AIs that are almost-corrigible will assist, rather than resist, being made more corrigible, thus forming an attractor-basin around corrigibility, such that almost-corrigible systems gradually become truly corrigible by being modified by their creators.

> There is also reason to suspect that almost-corrigible AIs learn to be less corrigible over time due to corrigibility being "anti-natural." It is unclear to me which of these forces will win out in practice.

### Remaining Dangers

> There are several reasons to expect building AGI to be catastrophic, even if we work hard to aim for CAST.

> Most notably, corrigible AI is still extremely vulnerable to misuse, and we must ensure that superintelligent AGI is only ever corrigible to wise representatives.

### Author's Retraction

> Edit: My attempted formalism failed catastrophically.
