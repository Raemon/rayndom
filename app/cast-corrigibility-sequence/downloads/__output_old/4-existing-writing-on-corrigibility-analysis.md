# 4. Existing Writing on Corrigibility - Concept Analysis

## Background Knowledge

- MIRI 2015 Corrigibility paper (Soares, Fallenstein, Yudkowsky, Armstrong)
- Arbital pages on corrigibility
- Project Lawful (plowfic by Yudkowsky)
- Paul Christiano's work on corrigibility and act-based agents
- Alex Turner's work on power, AUP, and non-obstruction
- Elliot Thornley and Sami Petersen's work on incomplete preferences
- John Wentworth and David Lorell's shutdown problem proposal
- Steve Byrnes' and Seth Herd's corrigibility writing
- GOFAI (Good Old-Fashioned AI) expected utility maximizer frameworks
- Utility indifference approach
- VNM coherence axioms

## New Knowledge

### Critique of MIRI 2015 Paper

> While I very much respect the authors, and am glad for the pioneering work, I look back on this paper as a bit of a misstep.

> The framing of the "toy-model" is a distraction, the focus on "methods of reasoning" over values/goals is likely wrong, and the emphasis on symbolic formalisms in advance of deep, intuitive understanding was probably stifling to the nascent concept of corrigibility.

> Corrigibility is a property that is necessarily about the agent desiring (as a terminal goal) to establish/respect/protect a specific relationship with its principal, rather than desiring that the world look any particular way, per se.

### Critique of Yudkowsky's Framing

> In my current conception, aiming for passivity is a dead-end, and the only robust way to get a corrigible agent is to have it proactively steering towards assisting the principal in freely choosing whether to shut it down, modify it, etc. This seems like a potential double-crux between me and Yudkowsky.

> Naming the thing "the hard problem" feels like it's smuggling in an overly-bold assumption that having a simple, central way to get corrigibility is hard and problematic.

### Agreement with Yudkowsky

> Even Yudkowsky seems to have an intuition that there's a simple, learnable idea behind corrigibility which, at the intuitive level, seems accessible!

> [Quoting Yudkowsky:] "The 'hard problem of corrigibility' is interesting because of the possibility that it has a relatively simple core or central principle - rather than being value-laden on the details of exactly what humans value, there may be some compact core of corrigibility that would be the same if aliens were trying to build a corrigible AI." Well said. This is indeed what gives me hope.

### On Yudkowsky's Desiderata (from Project Lawful)

> [The desiderata include:] Unpersonhood, Taskishness, Mild optimization, Tightly bounded ranges of utility and log-probability, Low impact, Myopia, Separate superior questioners, Conservatism, Conceptual legibility, Operator-looping, Whitelisting, Shutdownability/abortability, Behaviorism, Design-space anti-optimization separation, Domaining, Hard problem of corrigibility / anapartistic reasoning.

### Comparison with Paul Christiano

> At a high level I mostly agree with Christiano, except that he seems to think we'll get corrigibility emergently, whereas I think it's vital that we focus on directly training purely corrigible agents.

### Critique of Emergent Corrigibility (Christiano)

> [The author disagrees with Christiano's view that] "An agent which has well-calibrated estimates over its own benign-ness, and which is benign in expectation, will on average have a corrigible level of corrigibility."

> I don't see why we should believe that locally-satisfying-preferences (even when done perfectly!) will produce a mind which is corrigible. Corrigibility is not about preference satisfaction, after all.

### On Alex Turner's Work

> [Turner's] notion of power [focuses on] optionality... My intuitive sense of power is not exclusively about optionality, but rather I care about the causal pathway running from the values to the preferences being satisfied via actions.

> Non-obstruction [is] a fundamentally interesting and natural concept, though I ultimately think that it's not an adequate characterization of corrigibility due to being too weak.

### On Incomplete Preferences (Thornley, Petersen, Wentworth, Lorell)

> Incomplete preferences seems like a promising direction for research... a key insight is that once the button is pressed, it doesn't matter whether we end up in state b or state c, only that we shut down. And likewise, the button not being pressed implies that b and c are incomparable.

> I'm skeptical that [incomplete preferences] is, by itself, a full pathway to corrigibility... you might notice the absence of the concept of a principal.

### Overall Synthesis

> Much could be said about corrigibility that I've not covered here... rather than try to be comprehensive, I've focused on parts of the writing which seem particularly valuable for describing or naming aspects of corrigibility.
