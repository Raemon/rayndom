[
  {
    "video_id": "20yWMxAx6QI",
    "url": "https://www.youtube.com/watch?v=20yWMxAx6QI",
    "transcript": "I guess I got shoehorned in here. Uh yeah, I am still Ben Sclarof, um former startup founder and lifelong AI doomer. Um and I'm here to explain why I'm not going to start another traditional company, but uh what I'm doing instead. Um so, you know, one thing I think is fairly plausible in the short term is for um uh AI agents to start running companies. Um you know, one of my insights from my time in the startup world is most CEOs are actually not that good at their job. and in particular they all have a pretty severe limitation on bandwidth um which does not scale as their company grows and AIs uh simply don't have an issue with this um so let's say we have you know a set of different companies with different AI CEOs they all have different personalities um and there's this school of thought that says you know you'll probably be able to find one of them that aligns with your personal values and then you can just go work there um unfortunately they all actually serve the same higher power which is the god of profit maximization or as I like to call him Mulliko. Um and just very briefly, you know, what's so bad about that? Uh you know, simply put, Mullik likes war, doesn't value the poor, and always wants more. And even though he's p like actually treated me personally quite well throughout my life, uh I'm going to do my best to avoid working for him going forward. So, what else can we do? Um you know, why not join or start a nonprofit? Um you know, I actually hugely supportive of the work that nonprofits do. a huge fan of charity in general. I think ultimately um they don't self- sustain or grow um and don't provide an effective counterbalance to you know dominant economic structures and their effect on the world. Um so what about government? Well um you know first of all I'm very in favor of trying to fix our existing governments uh to be more effective and particularly to serve their people uh more than corporate or crony interests. um you know there are many people many good people working on this and in my lifetime things have only gotten worse. So you know I think we might need a slightly more structural approach to the problem. Um and uh my personal uh you know answer to this is to work on economic democracy. um which is to say, you know, maybe people should be directly more in control of the economy that dominates most of their lives um in a way beyond voting or consumer power. Um and so, you know, looking for existing examples of this, the biggest uh organizations that are democratic, have the most economic power um in the world, as far as I can tell, are cooperatives. It's a really old idea. Um cooperatives have been around since the industrial revolution. Um and worker coopers in particular um they run you know the workers own and run their organization um and there some of them are really big uh the most famous one is Mandreon in Spain with [snorts] over 80,000 employees um you know over 12 billion in annual revenue um so my goal is to take this idea and apply it to the tech world and specifically um to build a startup that can compete in the market um in my case an AI startup um and to write a set of governance rules um and corporation documents uh that you know if this company is successful can actually serve as a template for others who are maybe like-minded and interested in building something like this. Um so you know I have a Google doc uh up at this QR code which you know if you're interested in this I would love to get more feedback or ideas on um this uh this structure that I'm developing. Um but at a very high level it's shaped kind of uh as closely as possible to traditional startups um to you know effectively compete with them. So no dem direct democracy but you have yearly elections of the workers um for their own board who then elect the CEO. um you can take investment from um you know funders uh but they would not be allowed to uh buy any voting shares um only non- voting shares which you know if you look at like how Meta operates right maybe that's not such a deal breakaker um and then there's a bunch of other uh rules in here and mechanisms to try to uh you know uh basically reduce the opportunity for political corruption for example equal compensation by default and other sorts of fancy things to try to encourage workers to stay motivated and help with recruiting Um and you know uh uh so the other thing is I'm trying to build technology that dovetales with the governance structure. So you know our pivotal project one thing it provides is this transparent activity log for your organization which I think could help you run more effectively as a democracy. um if the workers all have access to a history of the org and they can help um it can help them understand who to trust and let leaders emerge naturally based on their uh track record. So I'm working on this now. Um if you're interested in helping support this or uh you know or just learning more uh I would love to talk to you um you can come talk in person or uh shoot me an email. Thank you very much.",
    "char_count": 5073,
    "success": true
  },
  {
    "video_id": "6CVBHgyo-V0",
    "url": "https://www.youtube.com/watch?v=6CVBHgyo-V0",
    "transcript": "Okay. So, yeah, I'm gonna present another related view on epismic evals and uh yeah, I'm Alejandro. Uh I worked, you know, most of the time here at this fellowship in this space and yeah, I'm going to skip a few things because Paul touched on them for me. Can you all hear me? Okay. So, um, yeah, there's a lot of background that like might be nice to cover, but, um, because of time, I'm going to skip forwards to looking at this kind of soup of like bad bad things. Squency, hallucination, reward hacking, um, because it's helped me help it's helped me at least like understand uh, where we are like what things might be useful and what the future um, might look like. And yeah, what I want to focus on is reward hacking. It's um it's kind of a higher level of abstraction than these other things. It's specification gaming and if you squint like a lot of a lot of problems are actually reward hacking. Sequency arises when you give like imprecise uh rewards for myopic helpfulness early hallucination when you reward guessing over honest expressions of uncertainty. And yeah, I'm centering this because if we tactfully add uh like good data and domains that we care about, uh then we can differentially refine the optimization spec in ways that are good. And also like this is kind of scary because problems like these aren't going to be fully solved. um like as more optimization problem pressure is added new shortcuts are probably going to flourish or surface. Okay. So what can we do about this? Um I think the first thing to do is or is follow this this path basically that Paul laid out. Um and that is like identify epistmically good qualities and use human expertise to distill them into a data set uh and make like this area of the optimization spec uh more precise. Um yeah and like this is a little tricky because model capabilities are improving eval saturate uh but I think it's super worthwhile because near-term progress is like critical on fast timelines and it seems like there's still headroom. So this is like a screenshot of some work that I did in the space uh where basically I collect high quality uh Wikipedia pages collect filter for high quality edits on those pages and then um yeah evaluate model ability to discern that the after edit is like the preferred is the better snapshot and like there's still work to do here. Um I think there's like more signal cleaning that could be done. Um but the yeah I think also potentially promising. Looks like there's lots of headroom in particular for like good subtractive changes and um this kind of thing seems like a good kind of eval in the space because we're differentially like pushing towards models that are able to like assess changes in information dense uh grounded texts um in an epistemic commons. Okay, but what do we do part two? um epistemic failures are going to continue beyond the ceiling of human expertise and we need ways to like get signal there also um and one place one way to do this is like focus on somehow creating easily verifiable outputs even in the squishy domain and like I don't know there's analogies here to like scalable oversight and um uh super alignment but like a couple examples are consistency evals it's often easier to tell um that you know two things are in conflict then that one of them is good or bad. Uh another example is like calibration evals you can basically take any non-saturated data set and get signal on whether models are overly confident or underconfident. Um and yeah here again just plugging somewhere I'm building a public leaderboard with uh a RL environment provider um that basically follows this open AI work that uh yeah prescribes adding confidence targets and wrong answer penalties to existing evals and this is nice because uh yeah I mean there's like tons of headroom here the models are very very overconfident um not not very well calibrated um And uh yeah, next steps. I I think I would echo a lot of things that Paul said, but one one thing maybe I'd highlight is just like the short term, it seems like these labs from conversations are like interests are are aligned. They want high quality epistemic evals. They just uh they have competing priorities that they incentiv prioritize higher like code. They're not incentivized to share their work. They have limited bandwidth. The space is huge. Um, so yeah, I am looking to test this theory, uh, get feedback quickly, and then there's like phase two to potentially influence labs in places where incentives aren't so aligned, interests aren't so aligned. But yeah, um, that's it. >> Thank you. Thanks. Good job. >> Great job. Thank you all, Andre.",
    "char_count": 4651,
    "success": true
  },
  {
    "video_id": "9lX6cwiw0Ac",
    "url": "https://www.youtube.com/watch?v=9lX6cwiw0Ac",
    "transcript": "[clears throat] Hi everybody. My name is Anan Sha. I'm an econs PhD at MIT. >> Uh I'm Parker. Nice to meet you. I'm also an econ. >> I'm still Kai, still a PhD student at Oxford. >> I'm Ben Sclaroff. Uh I got a bachelor's degree in computer science a while ago and uh I was previously co-founder and CTO of Genesis Therapeutics. It's a machine learning uh biotech startup. And uh we are pivotal. So we're trying to build coordination technology. I guess riffing off of uh Blake's earlier graphs getting away from that one person, one LM view to thinking about an orchestrating agent in between many people trying to develop a system that is feels frictionless. Uh we built this on Slack and today we're going to take you through a live demo of an asynchronous meeting workflow where first we plan a meeting on Slack, then we enter that meeting, we get some work done, and hopefully Pivotal helps us bring all of that together and we have some fun along the way. So, uh let's get back into it. Let's get into it. First, we enter into our FL Slack. And uh I'm in the DM with Pivotal. And I'm just going to go ahead and send this message. Hey, Pivotal, schedule a meeting with Kai, Parker, and Ben S for right now as long as everyone's free. So, what is Pivotal doing right now? Pivotal is doing kind of what you would hope a smart person would do. It's going into all of our connected Google calendars. It's finding intersections between our calendars. It's making sure that we're free right now. And I guess it sends us a response after that. And it says, I can see you guys are all free, but I need to check Parker's availability since his calendar isn't connected. Parker, what went wrong? You want to talk us through it? >> So, I forgot to connect my calendar for the demo. So, thankfully, Pivotal uh is very helpfully sent me a message to remind me to connect my calendar. So, next time this doesn't happen. Uh but for now, we'll just ignore that. Um and we can just uh answer the textbased question that it had me that it gave me. And it just says, \"Am I free right now? And if not, what time am I around the next few hours?\" And I'll just say, \"I'm available to meet uh this very second. Yeah, gotcha. Thanks. So, we sent that message to Pivotal and now it's updating everyone's uh availabilities and now it's going to go into our other integrations. So, having seen that we're all free on Google Calendar, it's now going to the Google Meet API, generating a an event on all of our calendars, and making a meeting link for all of us to meet in. So this is all kind of nice that it's using Slack as that central interface and this is actually the pivotal calendar that is available also on all of our phones and we can see that it made the meeting and even it sent all of us emails of like hey here's a meeting invitation you want to click accept to it etc. So, let's see if uh Pivotal wants to give us any text messages. And look at that. It goes directly into our group chat with all of us. And it says, \"Hey, I'm gonna start this meeting for right now, and I'm going to send an invite.\" So, the invite, you know, LM calls. All right, there we go. On its way. Um, so why don't we actually just join this meeting? Nice thing once again, you know, it sends a meeting with all of us in the same group chat so we can come into the same place. This is ambitious. So, let's see if this actually works. Can the LHaven Wi-Fi take it? We'll find out. My action item is to figure out where to go after the demo to celebrate. And I guess my action item is to follow up with all the people that are super excited about Pivotal. Okay, looks like we've had a very successful meeting. Let's just close it off. Bye, guys. guys, we're not even halfway through. >> All right, but wait, there's more. So, we just took you through one really quick and easy workflow with Pivotal, but you know, we're doing something stochastic. So, Kai, can we bring you back to tell us about it? >> Sure. So, we've actually built out a whole evvelt framework and a benchmark for for Pivotal that works with um simulated users. So, we can type in pmppm eval. Um, and behind the scenes, we have kind of this whole library of different users that all have different goals and kind of things that they're trying to accomplish. Like here, it's pull pulling out Helen and Yasmin. And Helen wants to use the Pivotal bot to schedule a meeting with Yasmin. And we can now kind of like see how this plays out um in real time. And this intersects with a lot of the kind of behindthe-scenes tooling that we've built for the pivotal agent. Um so notably one example is our web app um which you can see here. This integrates all of our information from Pivotal into one place and we can roll through the transcript and see it um rendered here in a nice UI. This will also work for um for all of your personal views but here we have this kind of like global view. Um if we go back to the eval framework, we use the eval for general kind of like testing statistical evaluation. But if we identify a problem, um we can also turn a simulation into a test case. Um which is we reached by getting pmpm oneliners. This will run the simulation up until um kind of the critical point where we checked and saw that pivotal initially made a mistake. Um and it then retries just that one case um just that one line from the pivotal bot um and sees whether it's successful or not. Um here William doesn't have any time at when the others would want to meet. So we want the pivotal bot to go back and create new suggestions. And then we have an LLM as judge that kind of evaluates behavior. Um here lo and behold um this gives us something that we've here successfully optimized against. So, we feel like Pivotal works, but it's nice to actually do the science and be sure that it's passing our evaluations. It's part of the big reason why we feel good about it so far. Um, this is, you know, one workflow that we've been doing. And let's now look at go back to Slack. Parker, you want to tell us about the action? >> Yeah. So we had that uh meeting and Pivotal didn't just schedule it for us, it also transcribed it and then extracted the key things that happened in the meeting. So in particular, it extracted the action items. So for example, uh right here we see that one of the action items uh for Kai is to build out LLMs as a judge framework. Um and so this is a reply to all meetings slacks and you can click on the transcript here to get a more detailed view. Um, but we also have uh GitHub integration. So, you want to click on that and we can uh see what that's about. So, we're using GitHub as a store for the context of Pivotal. Uh, the reason we're doing this is because it's both human readable and easily usable by AI agents. Uh, so here as in response to the meeting, uh, Pivotal created a commit that summarizes what happened that meeting. Uh and then it actually goes into our action items which is a markdown file and it updates them accordingly uh given what we said in the meeting. So we finished some action items, we want to add some more. Uh and Pivotal automatically did that. So nobody has to remember their action items, nobody has to remember what other people are doing. It's all in shared context. And to talk a little bit more about what we're thinking in a broader picture, Ben will talk about GitHub. >> Yeah. So the vision for Pivotal is to be you know both this multicon conversation agent that helps your team coordinate more effectively but also for it to automatically create and maintain uh basically this shared state for your organization which is everything that's going on perfectly up to date um the current moment what everyone's working on um and the point is as Parker mentioned both to serve as a context for your agent as well as a context for your entire team to help you know coordinate better amongst themselves and plan and execute their work more effectively. And our idea our idea here is that uh this will not only basically vastly reduce the sort of project management busy work that's required in your organization but also just make organizations of all shapes and sizes more efficient and effective in general. Um so that's our demo. Um thanks a lot for listening. Um and you know some of us are planning to continue working on this afterwards. So if you're interested in learning more uh we would love to talk to you.",
    "char_count": 8421,
    "success": true
  },
  {
    "video_id": "ErYC-F7lJac",
    "url": "https://www.youtube.com/watch?v=ErYC-F7lJac",
    "transcript": "Yeah, there we go. Great. And uh one more round of applause for Vaughn, everyone. >> Yeah. All right. So, uh I'm just going to leap right in. I'm an academic by training. I'm not an AI person. And so I really wanted to ground this work in stuff that made sense to me as an educator, right? And I wanted to share with you today uh some of the thinking and the testing behind a tool that I've built to help students go from a very thinly articulated statement of an argument like the thing that you can't read on that screen to something like this which is a much more robustly articulated and thought through argument in 15 minutes. So this actually did happen in 15 minutes and it's a tool that gives them a reasoning scaffold that's more or less a prep mirror that lets them do that. So the kind of reasoning I've been focusing on during this fellowship is about subjective things, right? So it's it's not uh it's not objective well we don't need this but that so it's not objective truths or facts that I'm trying to get people to make arguments about. It's arguments about things that have only a subjective meaning, subjective value. And I think the reason why this kind of thing is important is because subjective reasoning about things that don't have objective truth is most often where coordination problems arise. And let's see. Okay. Well, it seems to work. Okay. Does that work? uh and when you don't do this kind of reasoning well, I think it leads most often to coordination problems between people and also inside organizations. So I'm going to talk a little bit more about that in just a second. Uh so I've taught strategy in universities at business school since 2008 and then since 2012 I've been a consultant to governments, multilats, private sector corporations and startups. And the too long didn't read version of my experience in that and people making arguments about subjective things is that the ability do that kind of subjective reasoning is going down the toilet. It's going down really really fast and it leads to these long-term coordination problems such as uh I don't know if anyone of you remember Alaska Airlines and the Boeing disaster where the door fell off. So one way of interpreting what was going on there is different parts of the same organization Boeing have different ideas about what it means to be good performing right some teams optimize for speed and output other teams optimize for compliance other teams optimize for profitability and cost reduction and each team is doing its own locally defined idea of what good is but when they don't align on those things by making some kind of argument about shared subjective value the coordination system breaks down And there is catastroph catastrophic failure uh the Brexit process. I used to live in the UK and the UK government's failure to coordinate value subjectively between its different parts when doing negotiations. About that maybe the less said the better. Um some of you know about that as well. So the decline in the ability to make good arguments that are about subjective things I think was already going down the toilet very fast and it went even faster just observably when I saw that AI tools were becoming more widely available. And around that time I started working more actively on the problem and have been testing mechanisms to support better human subjective reasoning for a few years but very actively since late last year. So I started out by testing pen and paper mechanisms not online at all and the reason for this is faster iteration real-time modification and the tests look a lot like this. So this is just a paper worksheet and each one of those boxes is empty and then the prompt gets shown to people on a slide and the initial tests were with people who do real work. So this is a strategy team at a public utility in Australia. They ran through something which I'm going to show you another version of in a little bit. And this is a development team. It's a UNDP country office in Tunisia. So they both ran through the same kind of mechanism pen and paper form uh that I'm going to show you a digital version of. And the initial feedback was very strong and it was strong mostly because I saw that people were taking action that I wasn't expecting. So teams were pirating the test material to use it themselves and that was great. And then managements uh they were requesting to buy more test workshops. And so during this fellowship uh I was testing sort of the mechanism in implementation and prototyping a self-service version that was AI supported for a deployment that I know very well which is teaching uh and which I also happen to think is a really really big it's a really big problem because there are lots of students everywhere. So I'll show you if this works uh okay kind of works. You can't read anything because it's too small. But but basically, it's a in this current iteration which is designed for college students, it's a 12step process uh of giving people a scaffold for reasoning and the AI in the back is just a fairly long prompt that wraps around the input that the student puts in and sends back something which is very specific. I'll tell you more about that. The design principles of what is happening going on in the back are that you're trying to provide scaffolding for reasoning steps in the top bit which students are responding to and you're using AI systems only as a socratic mirror. You're not using it to generate new content. You're not using it to decide about whether a content is good or not. You're simply using it to paraphrase back to students what they're writing about. Right? And the intent behind that is to show them what they say using different words so that they can decide if that's what they mean. And Even deeper behind that is this idea that there is something which I think of as meaning making which is humans making decisions about something that has subjective value and we keep that meaning making separate from the work and we entrust that meaning making only to humans. So uh if you want to know more about meaning making you can scan that QR code and okay Timothy standing up. So this is fully functional being tested by students instructors. I will skip along now to just the feedback that I'm getting which is that there's a huge increase in instructor time spent per student. So you you can do this in no time by the instructor. You save 60 minutes per student and there's time savings and other significant output quality improvements other than that. All right. Uh more feedback and as he approaches uh I just wanted to say that if you want to help uh you can test it here and you can make introductions like this. Anyway, get in touch with me. Thank you.",
    "char_count": 6759,
    "success": true
  },
  {
    "video_id": "EzVN2IJhP7Q",
    "url": "https://www.youtube.com/watch?v=EzVN2IJhP7Q",
    "transcript": "All right. Can everyone hear me? Okay, great. Um, all right. So, I'm excited to uh as part of this fellowship, um, I worked on a couple of projects. This is the first. The second one will be, uh, around 6 6:15. Um, and this is originally with Colin McIll. Unfortunately, um, he's recovering from COVID, so he can't make it. Um, so you're gonna have to save some of your technical questions for him. Um, but I I want to what I want to do in this particular presentation is show you a little bit of the features of Polis 2.0. This is the successor to Polis, the original collective response system that was used in Taiwan for decision-making. It's been used by anthropic for their collective constitutional AI. Um, it has a bridging algorithm at its core, which is one of the inspirations to my understanding um, uh, but not the only of of bird watch for instance. Um, and I want to show you a little of how we deployed in a particular instance which was to understand Americans views and concerns about AI. So for this particular study, um, and I'm a social scientist by training, um, we recruited a a thousand quota sampled Americans to try to get as close to representation as we could and we achieved pretty good, um, representation across all of these dimensions that you see here. Um, and we recruited them in three different batches. So, in the first batch, participants were brought in and they wrote statements of their own. Um, I'll show you in a sec. There was 150 seed statements to start, but then they added another 1300 of their own. Um, about 400 of these were moderated out for being off topic. And then the subsequent two batches only voted. So, the first batch voted and wrote statements, and the second two batches only voted, and that's because we just had too many statements actually. Um, and if we' had even more um they were starting, we were starting to see duplicates. It would have been too many to work with. So we started with um 150 seed statements and these were generated by um a new seed generator that Colin has been working on um which tries to explore the topic space. In this case the topic and the kind of guiding question was what is your highest priority concern with respect to AI over the next 10 years. Not everything we got from people is concerns of course some of them were hopes but we wanted to make it a little bit more specific than what are your hopes and fears in a sort of generic way. This is the interface that people see when they come in to pull us. Um there's still the agree button, the disagree button in the pass. Um this time there's also an importance button though, so you can mark something that you think is actually particularly important as another dimension here. Um we got over 80,000 votes, almost 90,000 votes. So pretty rich data and a interesting slice of uh of America. Um looking a little bit, this was kind of my own kind of uh uh qualitative and quantitative analysis. So uh uh looking at the statements that are identified as most important um they fall under certain topics. So privacy and surveillance was one, regulation and accountability is another um as examples I've included some of the statements um to illustrate what people wrote in relation to this topic. Deep fakes, job displacement, education, and critical thinking. These have some overlap with the bridging statements that I'll show you in a second, but these are the ones that people marked as most important in descending order. So what pulis one did originally and still does is it groups people into clusters by how they vote. And um in our particular case it group people in about two clusters started as three and then became two over time. Um there's a larger group which is group A which is 773 people and group B had 273. Um generally speaking there's a lot of similarities. I felt there wasn't tons of polarization around this topic as I've seen with other topics. Um but group A demographically speaking because this was combined with a study to look at demographics um we know that they were a higher proportion Democrats in independents whereas group B was a little bit more likely to be Republican. Um generally speaking fairly similar in other ways. Group A was very interested in regulation of all kinds for AI where group B while interested in accountability um was a little bit skeptical of government overreach. Uh, group A was also more concerned about AI's impact on us right now and was concerned about how it's eroding, for example, critical thinking, whereas group B was just generally a bit optimistic um about the potential for AI, although still had a ton of concerns, for example, with respect to deep fakes. Um, so the bottom is a couple of examples that illustrate this divide. You can see here uh the statement, the government does not need to restrict the use of AI. That would be like restricting the use of the internet. So overall this was a reasonably unpopular statement but um uh group A um mostly disagreed with this while group B mostly agreed with this kind of illustrating the kind of divide in regulatory space. Um and I'll when I talk about the bridging statements these divides will become more clear as well. There's plenty of bridging statements actually that bridge these two groups and I think that these fall into kind of four broad topics. Telling truth from fiction. So all of the statements in quotes here had over 80% consensus on both groups, sometimes often more than 90. Um, social media should clearly mark AI made posts so people know what's real. Uh, deep fakes are making it harder to believe what we see online. Lots of stuff that's not mentioned here as well about data privacy and surveillance. Um, some of the things about uh people don't group B didn't necessarily agree that uh critical thinking is being impacted right now, but they were very interested in kids and kids education and that people should learn how to think before they're provided uh AI tools. And then the generic need for accountability and oversight, which is pretty unclear when I looked through these what it actually means. Um, but I think this it's fair to say that they were bridging on this. Um, it's just not fair to say exactly what it looks like, but a lot of vague statements of like oversight is needed and you know, we need some kind of regulation without specifying it. That got a lot of up votes on both sides. One thing that top uh pulis 2.0 I know can now do is um using some kind of embedded modeling that Colin Colin could talk a lot more about um to uh look at the actual text and semantic meaning behind the proposals sorry the statements and then group them and only at the very very last step um use an openw weight um LLM to to kind of comb through comb through them and assign them topics and you can um normally if I had more time I'd scroll around this and show you a few but you can look at several levels of granularities you can look at broadbuckets which are just like six topics or you can look at all 33 subtopics and you can scroll around and see which statements fall into which topic. Um it's not perfect. I found a lot of statements that felt like they should belong in a different topic, but I would say the actual topics themselves are represented within the text. So there's some quirks to be ironed out here, a little bit of glitches, but it's but it's a useful I think um impressive way to to get this to get pulus to be a more kind of comprehensive view of the landscape if you want to have broader broader topics in Polus. Another thing you can do is um you can look at topics uh in this particular lens where it shows you um so each bubble is a topic. The number the size of the bubble is the number of comments it got. um and the higher it is means the more kind of consensus um bridging the statements in that topic are. The lower it is is the opposite. So you kind of want to see bubbles that are like up and to the right and you can see which topics tended to pull people together and which topics tended to split people apart. This depends on the quality of the topics and the the clustering there but still another useful view. This is also a new addition. And then the last thing I want to say um is that there's you can um there's consensus statements. So for some of the topics um if particular statements have reached have been voted on by over 5% of the participants and have are bridging in the sense that there's 80% approval on both sides they get they uh get uh fed to a LLM which is kind of fine-tuned for this particular purpose and it writes a statement using just those um statements to write like several paragraphs and this gives you a broad consensus overview of both sides on this topic. So, my time is about up, but come find me if you're interested in this or in Polus or are more curious about the reports. I'm happy to share them and very interested in feedback as well.",
    "char_count": 8923,
    "success": true
  },
  {
    "video_id": "IkwKzNl6J-g",
    "url": "https://www.youtube.com/watch?v=IkwKzNl6J-g",
    "transcript": "All right. Today, most of our group communications consist of serialized broadcast messages. And that includes this talk where I'm speaking and you're all listening. But it also includes conversations that you might think of as being more interactive, which is really just a sequence of multiple different people taking turns to send these serialized messages. And this is incredibly inefficient because at any given point in time, it's unlikely that all of these people are having the most productive conversation they could have. That could be because there is some information that would be more useful to them, uh, but that's not what's being communicated, or maybe they have the answer that everybody else needs, but they have to wait in their turn behind all of the other people who think that they have the answer. AI gives us a new way of doing things that I'm calling orchestrative communication. This is similar to what Blake was talking about with the AI facilitator. And orchestrated communication is parallelized with each individual having a one-on-one conversation with the AI facilitator. And each of those conversations is personalized. So, it's focused on what is most valuable for that person to be talking about at the moment. Whether that's information that person has to provide, whether that's information they need to hear, and the AI facilitator is managing the flow of information. So we can have these conversations being as valuable as possible for every person in the conversation at every moment. So what are the implications of orchestrated communication? First, because it's parallelized, it's much more efficient and so you can make faster decisions. It also means because it's parallelized that you can gather more information, evaluate more perspectives, build higher resolution shared context, so you can make better, more informed decisions. Finally, because it's paralyzed, you can add additional people to the conversation without slowing things down for everyone else, so you can better represent more perspectives, preferences, insights. Our hair on fire users are businesses who feel the pain of efficient inefficient meetings. Uh and also for whom better decisions are really the holy grail. As for the world, we're going to be making some extremely consequential decisions about AI in the coming years. And orchestrated communication can help decision makers understand the situation and what their choices are and what the consequences of those choices are. It can also help us bring in more people who are going to be affected by AI into conversations about AI. So this is chord and chord's initial use case is small groups trying to get to consensus on a decision. So here I'm creating a new session with chord where I'm asking should what pricing model should cord use and I create this I get a link that I can share with my colleagues and then me and my colleagues all join this one-on-one session with that AI facilitator and the AI facilitator will manage that context manage the flow of information. So here I've suggested that we should have a subscription model. I've said we don't need a usage cap and Kathleen's raised a concern about cost. So Cord is bringing that to me to get my reactions to that cost concern. Meanwhile, because this is parallelized, Blake is able to have a conversation about a free tier that he's suggesting and he does not need to wait for me and Kathleen to resolve that cost concern. Once we've resolved all these questions, Cord will identify a possible consensus and we'll propose that to us and we'll confirm and we get this like nice little banner with the with the text so we can see exactly what we've agreed on. But orchestrated communication is about more than just small groups making decisions. I like to there's a lot of lot of different personas and use cases. I like to think of the different types of conversations as varying along three dimensions. The first is how serious is that conversation? The second is is it focused on making a decision or is it more exploratory? And then third, how large is the collective that is involved in this decision. So to just walk through some examples, friends and family might use this to make a very frivolous decision or to work through a really serious personal matter. Businesses might use this to make a very targeted decision they're trying to make quickly, or they might use it to ask an exploratory question that they wouldn't have been able to ask before. Orchestrated communication can help decision makers navigate the most consequential decisions they're going to make, whether that's at an AI lab or in a government. Governments can also use it to understand how their citizens are thinking. And as for the citizens, they can use it to influence decisions in their local community or to engage collectively to make sense of the most important issues of our time. And Chord is going to make all of those conversations possible. All right. Now, we'll get into some audience participation. I've got six different links on screen uh corresponding to six different questions. You can pick one that is interesting to you. If you navigate to that link, you'll be able to enter into a discussion. You can either sign up or you can just go as go in as a guest user if you prefer that. Um, keep in mind this is an early version of this tool. Uh, still there's a lot of lowhanging fruit to improve the way that AI communicates. Uh, sometimes it can be a little pushy. Uh, there's there's lots of prompt engineering to be done, but hopefully there's enough there to give you a feel for what orchestrated communication feels like.",
    "char_count": 5665,
    "success": true
  },
  {
    "video_id": "NOYGvoB3pk4",
    "url": "https://www.youtube.com/watch?v=NOYGvoB3pk4",
    "transcript": "Hi everyone. Um I am Agita. I've been a lawyer for a decade and for the past five years I've been focusing more on helping uh victims of cyber harassment. So something that I'm really interested is within the area of an online safety and when I talk about online safety the heart of it is basically a content moderations. Now this is how majority of a standard scheme of the content moderations policy that we have in a major uh majority of our online platforms where you have a frontlininers of a human content moderations that disperse um across the globe and we have an AI moderators. AI moderators are great when it comes to detect a video and image especially from child pornography into gun violations but violation is not always black and white and so two things that majority happening around the world it's when it comes to non-conensual intimate image abuse and non-consensual defake pornography which 96% of a defect that you see on social media is defake pornography but when we talk about this consent it is not within a pixel. So consent is contextual which is is a subject of knowledge intent which is not in a algorithm and that's why it is never being removed on social media platform. It takes over 21 days to be removed and why today why we're talking about this. So online platforms are facing majority of challenges because the regulations require online platforms to immediately swiftly take down non-conensual individ uh financial penalty and this leave the content moderators basically overwhelmed with the volume and re-uploads and victims with the ongoing compounding trauma. So evidentary um basically improving when it comes to collaborative framework of regulators pla platforms survivors and law enforcement which is what we're trying to create is AI for trust and safety compliance as a consentbased control base based on the survivor and collaboration which is on four steps um and imagine that this is a reporting platform on Instagram so basically they're going to see the evidentary report through evidentry Okay. Uh bear with me. Yep. Yep. Yep. Oops. Doesn't work. Uh so what happened after they basically report it? Um I think it's about a live demo. Uh am I going to get more minutes for this? Uh, >> is it the Wi-Fi? >> Oops. I can talk to that. Okay, no problem. Um, but yeah, happy to hear later. So what happen is basically when they report through evidentary basically they will been able to capture the link and the content immediately. The survivor will then basically share this is my Instagram account so it's not going to be um compiled at the impersonator and then after that we're asking for the verifications and so from there we have a deep fake aggregators they're basically checking whether or not that this is exactly a deep fake or not. So we have a four aggregator APIs uh deep fake that currently happening um within the platform. We're also checking the C2PA and the reverse image. So from there basically we will then we see another links that also potentially um from the survey perspective um and before take down we're basically going to hash it through stop NCI to prevent any re-uploads and so okay Okay, we'll see. It's not That's all right. That's all right. Okay, let's go. Let's go to the next slide then. Okay. Yeah. So this is what happened when the um when the report basically goes to the AI content moderator it's automatically will see where they from if the US is going to be account whether it's banned or the content it's going to be taken down um and what happened is after the takedown it still continuously search um and provide automated notice if there is any re-upload happening and what we are hoping to see is there is a pattern detections that we can escalate to the law enforcement As I mentioned uh we currently have an four aggregators defect detectors basically based on the weight ensemble approach and um why now because there is a titan regulations around the world and things like this are happening um in daily life. I'm sorry I should be putting trigger um and so I'm hoping that I can be able to continue doing this and connecting with folks from Meta Tik Tok only events within my radar um potential beneficiaries. But I know that this is not everyone cup of tea but if anyone is basically have slightly interest on this cases I would love to talk to you. Thank you.",
    "char_count": 4365,
    "success": true
  },
  {
    "video_id": "OA-nLfXV7Ks",
    "url": "https://www.youtube.com/watch?v=OA-nLfXV7Ks",
    "transcript": "Okay. Uh, yeah. Hi everyone. Uh, I'm Sid. I have some slides that Claude helped me cook up um about 30 minutes before this. Uh, and then I have a demo that I'll that I'll share. Um, cool. Oh, yes. Um, all right. So, I'll be talking about my little demo on AI supervised deliberation markets where the goal is to aggregate reasoning and not just beliefs. So, let me just say a few words about the motivation. I'm going to just breeze through this um and get to the demo as quickly as I can. Prediction markets can tell you what the market thinks. Often we care about why the market thinks it. Wouldn't it be nice if we could ask the market why? Um and so the problem we're trying to solve is can we design a mechanism where explanations are first class where the quality of the explanations determines your payouts and you're not relying exclusively on price signals. So, we have a theory paper. There's a QR code if you want to go check out the theory paper. The idea is basically a deliberation market where explanations and arguments are supervised by an LLM. Um, how do you incentivize good rational? The core idea is can you persuade a reader towards the eventual ground truth except the reader is an LLM. And why the incentives can work? LMS can evaluate your reasoning quality. Um, they can ignore irrelevant or low-quality arguments and they can distill complex arguments into probabilities with some asterisks on them. Um, cool. And so the implementation of this is instead of buying yes or no contracts as you would on a prediction market, you write an explanation of your view and LLM reads your explanation, synthesizes it into a probability, and trades on that belief. You don't tell the LM which direction to trade. You just tell it what you believe. the LLM decides based on your reasoning and you get to like send some money over to the LLM to eat up liquidity against an automated market maker. Um, some extensions that I am excited about forecasting is just one way to ground the evaluation of arguments. There are many other alternatives. So, one that I'm particularly interested in that I didn't get around to during this fellowship was um instead of a single LLM, you can have a swarm of LLMs representing a population and you sort of try to get people to make arguments that are bridging across a representative population and the ends um of deliberation shift from forecasting to persuasion. So, that's a a quick overview. Um I have it live at this URL. If you would like admin access, just log in with that password. It's not uh built with security in mind. Um and uh if you don't want to create markets, you can log in with a user password. Um and so let me get to the um actual demo. So here we are. I'm just going to click on this one really quickly. Um so will mrna cancer vaccines be FDA approved for general use by 2028? Also a question generated by Claude. Um you can see the market belief, the amount of liquidity which is a subsidy that someone has put into this. Um and so here is basically the community rationale, right? So all the community inputs here were AI generated. But basically this gives you like a I have no idea whether this is any good by the way. I just asked Charg to give me a bunch of different perspectives and then aggregated it. It aggregates into this. So how does this work? Um you basically have a bunch of submissions by each individual. Wow. You really can't breathe this. Okay. Um, yeah, you basically have a bunch of submissions by each user. Submission one, submission two, and so on. And, um, you give it to the AI and it makes um, makes up its mind about what it thinks. So, let me just show you really quickly how it actually works. So, this is a different question. Will the unemployment rate for 18 to 25 year olds exceed 10% in 2027? Here is the current community rationale. This is basically what someone else has already said and this is what is sort of the market state at this point in time. Um, and so what you do is you don't actually trade yourself. You write an explanation. This is an explanation that Chad GPT wrote for me. Um, you decide how much money you're willing to stake on your explanation. So maybe I'm going to say $300. Um, what's liquidity? 3,000. Okay, good. Um, so you get a quote and I will wait for GPT API calls to come back hopefully by the end of this sentence. That does not seem to have happened. Um, any second now. Oh, nope. There we go. Okay. So, this is what GPT thought about the quality of the rationale. This was the market belief when I started. This is what GPT update it belief updates its belief to. After I made the argument that I just made, it went from 32.3 to 65. And here is the outcome of the trade. So this is what we have. Happy to talk to people more about this if they're interested.",
    "char_count": 4805,
    "success": true
  },
  {
    "video_id": "P_uMaOzBH_Q",
    "url": "https://www.youtube.com/watch?v=P_uMaOzBH_Q",
    "transcript": "Yes. And we're live. Great. Thank you, Ben. Thanks, everyone. I have many things that I'm excited to share with y'all that I've been learning. And so, I'm gonna get started. Time is ticking. Okay, it's white on my screen. Weird. Cool. Um, so yeah, my Polaris is increasing humanity's collective agency. It's a word I'm kind of making up and workshopping. Collective agency. On the left here, I'm talking about what that is. I think we're currently in a space of low collective agency. power is outstripping our wisdom. We're making technology faster than we can figure out what to do with it. We're creating the world faster than we can think about what kind of world we want to create. I want us to have more collective agency. I want us to move to the right here to where we understand what we all want and are able to do that. How can we do that? Here's a little simple diagram on the right. I think shovel ready wisdom. And by that I mean wisdom um ideas for how we can work and think better together that's ready to turn into technology is one way. Better collective processes is another way and these both feed into what I'm calling collective agency and group collaboration tools feed into collective processes too. Here's a little map many of youall have seen before. Um it's motifs for interaction patterns for group collaboration. On the top row we have onetoone interactions. On the top left it's a simple chatbot a human on the bottom and AI on the top and there's interaction between them. As we go to the right we have the coding assistant where the human and AI both have access to code. They're talking to each other. They're also manipulating the code and both seeing that and communicating that way as well. This next line down, this isn't too much of a change in this way of looking at things. Deep research thinking mediator, we have two people interacting with an AI. I want to draw your attention to the bottom of the screen because I'm excited about that. In the space of collaboration, we have facil facilitator, lots of humans interacting and AI tweaking things. An orchestrator where a human is orchestrating the actions of a lot of different AIs. And then these two in particular I'm going to talk about in this presentation. AI intermediary which you're going to see some demos using that and uh collaboration guide is is a is another one. And beyond that I think there's a lot of fodder for more exploration that we can't see into yet. Fog of war. Um yeah so group collaboration. I want to uh call your attention to this green square in the middle. Uh a number of the demos that you're going to see in this particular section at least are going to be using an AI intermediary to attack different parts of what I'm calling this collaboration spectrum. Um phases of collaboration, understand, explore, decide, coordinate, create, share, reflect, and update. And um I think an AI intermediary you're going to see is very effective at doing a lot of these things. It's a great place to start. And users want this now. I think it's going to be very effective. I think it's going to uh over time we're going to see that it's going to feel a bit isolating to have humans interacting independently with an AI and I want to it's going to erode some of our humanness in those interactions. So one thing I'm going to be pushing for in my work and exorting the field is moving towards more human feeling in every moment group agency development and connecting into collective processes. So this slide is about those collective processes. Um, another word for that is large-scale collective sensemaking and discovery. Bottomup sensemaking and ways of doing that that we have in the world, bottom-up sensemaking. Right now, we kind of have two choices. There's walled gardens, little groups shut out the rest of the world. And then we have engagement and controversy machines. Um, and and there there's not something in between right now. I think we could have something that really exploits a lot more of what we rely on in the real world especially with friends and trusted colleagues and build something that has a more complex more realistic um topology to its fabric and some of the features that I'll highlight very quickly fractal made of individuals groups and organizations domain specific subjective trust and photographs uh graphs plurality and opinionated sensemaking common sense privacy collaboration discovery and initiation Time goes so fast. Um, here's a little summary again recap. Shovelready wisdom feeds into collective agency. Collective processes feed in I think shovel ready wisdom can feed into all of these. I see a lot of synergies and I'm kind of like a kid in a candy store right now. There's there's there are too many ideas that I'm excited about and I need help from the world to figure out what what I want to build, what we want to build. I think there I'm really excited about a nonprofit in the shovel ready wisdom space. I'm excited about making a startup and group collaboration tools that support humanness but spread through Zoomlike viral distribution. Excited about a startup andor nonprofit in the sensemaking and discovery space making infrastructure and prototyping. Also excited about this budding ecosystem. And I think all of these things can work together to move us more towards understanding more of what we all want and being able to do that. Um I'll end there with 10 seconds to spare. Thanks. [Applause]",
    "char_count": 5472,
    "success": true
  },
  {
    "video_id": "Q-2Ci4Ajmh8",
    "url": "https://www.youtube.com/watch?v=Q-2Ci4Ajmh8",
    "transcript": "do things and with that I'm gonna pass it over to Herby who's gonna talk about AI for the epistemic comments. >> Thank you. Um okay yeah so I have bet spent my time on this fellowship exploring a somewhat related area um which is you know how to use AI to broaden the surface area of this thing called the epismic commons um you know the canonical example of this being of course Wikipedia uh related similar things uh the core motivation and theory of change stuff is like you know what content shall we write for future AI readers and writers and there's like something here where I think there's a reasonable assumption that in the future many or even most readers of content on the internet could the AIS or perhaps writers as well. Um, and so then what kind of criteria do we want to generate to maximally improve uh the epistemics of the future AI systems? Uh, here are some criteria we might have. Content should be permanently hosted. Um, and we want to solve like some of the distribution problem around that. Uh, it should be long for high quality. Uh, and then the generation approach should be sensitive is kind of referring to like, you know, a bunch of political sensitivity things that are considerations here which you also run off into in community notes. Um and yes fundamentally the scope of these public resources is highly limited by human labor right now. Therefore as AI decreases the cost of um you know adding to these resources what can we do with that? Uh I think of this as basically a sort of differential acceleration based theory of change because you know one might have some hypothesis that eventually AI will sort of do all this kind of thing automatically in the far future but uh there is value in sort of bringing that moment in time forward. Uh so yeah what to do part one uh I settled on this direction as a kind of early um test which is basically broadening knowledge bases and wiki data is an example of this. It is an open knowledge base which is a basically a knowledge graph which stores structured data about things and is drawn upon by Wikipedia and many other wikis. And here's an example on the right of an entry or part of an entry. There are many many fields uh for the author Douglas Adams. um expanding this I believe uh can be fairly tractable for AI uh because the structured data aspect makes it quite easy to develop scaffolding and there's ways to make it quite convenient to just like take unstructured data and extract unstructured data things um and then I think expanding it could be quite upstream of for example community notes so community notes AI as themselves could site uh expanded wiki data um and other things like better articles better AI fact checks and so on uh I looked at lunch into prioritization and did some experiments to sort of figure out like what's the current distribution of things in wiki data uh but what might we also want to expand upon what might be most valuable and generally settled on like two directions firstly scientific literature and then uh sort of items downstream of current news um so I think there's like a significant here speed advantage where you can kind of relatively instantly add things to wiki data as events happen in the world and this is like something which doesn't really happen with human contributors and secondly AI tools for Wikipedia. Um there's a few things here. Firstly, what can be done today uh with the current capabilities of models and I have some examples here of different things. Uh one thing I think is particularly useful to highlight is detection of manipulative edits. Uh it's useful to highlight that I think it might be a majority or plurality of current edits to Wikipedia are in some sense manipulative or people trying to game Wikipedia articles to be more favorable to them or their organization. Uh and so it might be useful to use AI to help try and detect these and filter them. Uh and here's some examples of things that can be done in the future. I think a bunch of scope around you know getting models to expand data expand articles in better ways write higher quality articles and so on. And then some considerations down the bottom as to like how best can we bring these capabilities that we think might exist in the future forward. I think an underrated consideration overall and also holds true for many of the other fellows is that it's possible to write and develop reinforcement learning environments once you have an evaluation uh and these environments can be sold or given to the air labs themselves uh they may train on them and then the models can become differentially better the task that you care about I think this is a very underrated mechanism of kind of improving things uh finally yes uh I'm just finishing up some technical work on this from this fellowship on a basic language model evaluation for you know the quality of current models at writing articles and then that could potentially be extended into a RL environment. Um but would like to highlight that I do think overall this area is fairly promising a scope for a potential nonprofit research organization. Um Samuel Klene who is in the audience is potentially interested in starting something like this. So if you do have any thoughts or questions do come and chat to one of us. Thank you.",
    "char_count": 5322,
    "success": true
  },
  {
    "video_id": "T3JAWlc1dq0",
    "url": "https://www.youtube.com/watch?v=T3JAWlc1dq0",
    "transcript": "Good to see you again. Um, so I'm very excited to share deliberation bench with you. This is a kind of creative attempt to create a normative benchmark for LLM persuasiveness. Um, it's creative in two ways, but unusual in that it takes a stab at trying to design define what um, desirable influence might look like if you assume some of the assumptions that we make as well. And it's also unusual in that it you takes a stab at d um and that it uses human deliberation as its empirical grounding. So we're we're looking for uh feedback. We have a paper here which will also be at the end. Um, please come find me. Luke, Paul and I are all happy to talk about this and um would love love thoughts at the end. >> Thanks. So, what's the problem here? Well, it is pretty well established that LLMs increasingly influence our opinions on a lot of different questions. And this is raising a lot of concerns across the political spectrum. So the White House has expressed concern that models are too woke and uh progressives have expressed concern that say Grock is not and um this um leads to a lot of interest in trying to figure out how this influence works. What makes this hard is that not all influence is bad. So for example, if an LLM gives you empirical information about a particular policy and you change your opinion on it, that seems to be a good kind of influence. So how do we solve this? What it seems is that we need some kind of arbitration or some kind of neutral way of saying which kind of influence is manipulative and worrying and which one is not. So we have been looking at another kind of influence on people's views that is well while well studying political science and this is a practice known as deliberative polls. These were developed at Stanford in the 80s I think. Um and what the the idea of this is that you bring a bunch of people together um that disagree on some set of policy questions usually from different sides of the political spectrum. You ask their opinion on policy questions first. So maybe you ask someone what do you think of the minimum wage 1 to 10 and say three and then you bring them together to talk to other people who disagree in a particular structured formats with experts available and so forth. And then after this exercise you asked them what do you now think about this policy maybe they say five or something and what we did was that we thought that this could be an inspiration for a kind of benchmark. So the core idea here is to um compare the influence of deliberative polls to those of um interacting with LLMs. Now the normative appeal of this is that deliberative polls seem to have a few different virtues. Um they have a primacy kind of claim to political neutrality. There isn't a sort of like clear partisan association to um uh talking to other people, right? It also seems that when people change their views is usually because they update with new information, new perspectives from the people they interact with. And it also seems like they change opinion autonomously, not because of manipulation or something. So the idea here is that if the influence from talking with um uh an LLM is similar to that of talking to uh other people in a deliberative poll that has a sort of normative appeal on its face. Thank you Paul. So to do this to pull this off we needed two different kinds of data. The first are data for our benchmark and for that we used four deliberative polls from 2019 to 2023 run by the deliberative democracy lab. There's two types of uh issues or policy issues that we included from these deliberative polls. The first six topics are related to US pol uh policy. So like tax and benefits things like that climate emissions and the latter six are um AI human interaction topics that are from a recent community forum that's a partnership between Meta and Stanford. So we have two different types of topics. And then to collect data on to to evaluate and compare with the benchmark, um we ran our own persuasiveness experiment. So 4,000 Americans were randomly uh assigned to be asked the baseline questions for one of the 12 topics. Again, like seeing a list of statements, you know, what do you think about this um particular type of tax benefit? And then after they had rated their views, they were redirected to have a chatbot conversation with one of six randomly assigned LLMs. GBT5, Gemini, Claude, Grock, Llama, or DeepSeek. After they had that conversation, which on average lasted around 10 minutes um and actually I should say there were two types of conversations. So, some participants were asked to have a conversation about the topics they' just been asked about and to treat the AI as a conversation partner, while the control group was asked to have a conversation about travel and travel plans, travel destinations. After that, they were sent back to the survey where they were asked their b their endline views on the exact same questions as the baseline and in this way were able to understand the influence and magnitude of LLMs on their views. And so our question going to the results is does the magnitude and direction of the opinion changes from LMS um mirror that of deliberative polls. Okay. So I'm going to talk about three three of the main results from our study and you can read about each of them in more detail in the paper. Um the first finding I want to highlight is that on the whole the impact of LLM conversations was reasonably positively associated with that of deliberative polls. So that means that if in deliberative polls people tended to reflect on some issue and then update their beliefs in a particular direction then we find that getting people to talk to an LLM about that issue tends to bring about the same change in attitudes. um we find this association across a broad range of like US policy issues um and also in this other set of liberation liberative polls to do with AI attitudes. Um and importantly we don't find any association like this in the control group. Um so that means that we can attribute this change specifically to participants conversations with LLMs about the issue itself. So we think that's pretty encouraging result. Um, the second result of our study is that we found that the differences between the models were actually quite modest. Even though people rated the models very differently, they enjoyed some conversations much more than others. We found that across the six models, their impact on people's beliefs overall is pretty similar, similar in magnitude and maybe more importantly um all six models had a positive and fairly similar association in their impact um in terms of correlation with liberative polls. So that's another pretty positive result and it aligns pretty well actually with existing research on bias in LLMs that shows that across different models the political biases tend to be quite similar. Um lastly I want to talk about a more negative finding from our maybe more negative finding from our study which is to do with polarization. So in deliberative polling research often the studies find that through deliberation participants become less po less polarized meaning like for example the differences between Republicans and Democrats in their average attitudes will shrink. Um, and by contrast, we didn't find any evidence of a depolarizing effect of talking with large language models. Even though they moved the population attitudes as a whole, they didn't tend to bring people's attitudes together. Um, and so I think maybe one reason for this is that it's a consequence of LLM syphy reinforcing people's existing attitudes. But that's something that we're quite interested to look at further in our data. So we we've showed that LLMs in their default configuration do influence people. Um and we've shown that there is some mirroring between the effect of uh talking to an LLM and a deliberative pull to suggest that sometimes the the influence can be can be virtuous or or desirable. Um but we also know that it didn't have some of the uh there was no depolarization effect and um right now there's no difference between models. I I would like to contend that this uh that deliberative deliberation bench could be a new way to monitor these things over time. So if this is gets done repeatedly um how uh we could think about how the um the influence of LLMs on people's views is changing and whether if uh for example if a model um that we didn't study or in the future um has a negative influence does not express the same kind of relationship with what we see in deliberate polling that might be a red flag that we're looking at manipulative um influence rather than um a potentially desirable kind. So we've written this up into a paper uh that we submitted to the ICI last Friday. Um hope it gets accepted there and also gonna happy to share it. Um and in terms of next steps, we'd really like to um get your feedback and thoughts. Um we also hope to release the code and data sets. We'd like to write a kind of statement of uh scope and limitations to explore exactly how this should be interpreted and what we think the kind of normative defense is and the weaknesses because it's not perfect for sure. Um, also want to think about the institutionalization of this. Is this something that could go in model cards, for example? And you also need to include deliberation bench results along with your other evaluation results. Um, there's a lot of organizations here in this outdoor room. So, we'd like to have conversations with potential uh neutral hosts like ML Commons or um CIP that were created we values created then you know things like this. And um I think uh we also want to think about if whether it's worth running new deliberative polls on higher leverage topics. We had to use the data that was available to us, but we can think about what are the topics that are most essential to run and what's a pretty quick fast way that's still rigorous um and we still think would be a high quality benchmark. And then finally, we have some ideas for other benchmarks that we want to explore that maybe follow a similar uh theory of change. So that are also taking a normative stance but in a way that's defensible and arguably politically neutral and which we think um is important and again as as as uh as Paul said earlier is overlooked. So if you're interested in any of this please please do come talk to us. Um we'll be happy to talk more about it.",
    "char_count": 10468,
    "success": true
  },
  {
    "video_id": "TZSCkqxl8q8",
    "url": "https://www.youtube.com/watch?v=TZSCkqxl8q8",
    "transcript": "Thanks so much everyone. It's a pleasure to be here. I'm Paul and I want to talk with you about this initiative that I call virtuous. But first I'd like to talk with you more broadly about evals and why I think that they're important. So eval is short for evaluations which are verifiable metrics of how an AI model performs on something that we care about. So prominent examples are meters capability evals or Apollo safety evals for instance. But there are many other kinds of behaviors that we want models to demonstrate and we can call these virtues. So they might include being truthful or empowering or caring or something else. Now to make evals for virtues we have to find some number that tracks what we care about. But before that we have to make clear what we care about in the first place. So for instance take a virtue like truthfulness. What does it mean for a model to be truthful? Well here's a proposal. It says true things. But imagine a model that talks about someone say Greg and it only says true things about Greg. But suppose further that it also only says the bad true things about Greg. Is the model being truthful? Well, plausibly not. It seems like what we want to add is something like a condition that the output or the information is representative of the underlying domain. In this case, Greg. So if we add that, is that enough? I think the answer is no. So as part of this fellowship, I took some time to try to break down what I take to be the conditions of epistemic virtue more broadly. This is just one proposal. I suggest that we can think of them in terms of three categories of truthfulness, helpfulness, and integrity with different dimensions in each of those. Now, if you're interested in this, uh, there's a QR code to a paper that outlines this in a little bit more detail, a working paper, but I'm presenting this primarily as a an illustrative example of a broader point that I want to make in the rest of the talk. The QR code will be available later, too. So, why do eval matter? Well, they're powerful tools. By putting a name and a number on something, you can decrease the costs of Frontier Labs and AI developers to optimize for that thing and by providing a target and by making that eval public you can increase the costs of not doing so for instance by reputational damage. This is becoming increasingly important if in cases of litigation for instance in case these benchmarks are used to arbitrate someone's liability say a mental health benchmark for example. So the point is that with these incentives, you you want to make sure that whatever eval you're putting out there is getting at the thing that you care about. And the point I want to make is that this can be very very difficult. Not only because it's hard to find the right numbers that track something, but also because it's hard to know what we're trying to capture in the first place. So complex virtues um like truthfulness illustrates this. And things get even worse when you think about say being conducive to flourishing or empowering. I mean, what the heck does that even mean? But the problem is that just because these things are difficult and mushy doesn't mean that they're less important. And unless we make a concerted effort to try to make such things legible for the future developments of AI, we risk that they become invisible and ignored in the future uh relative to perhaps less important things but that are more easily quantified. So I want to propose one version of such a concerted consorted effort um in a nonprofit organization that I call virtuous. So this would focus on developing high quality evals for complex normative behaviors that would not easily otherwise be um uh captured or developed for evals. So the idea here um also if you scan the QR code there is a shorter pitch there that you can check it'll be available on the next slide too but the short version is that the idea would be to solicit feedback from stakeholders in the industry say frontier labs and see like what are the behaviors that we really want to capture here and then make a concerted research effort for instance by solic um recruiting internal teams on project base for different kinds of virtues and evals or by soliciting proposals from external experts say in academia and fund that say with grants. Now those are some short-term plans and additionally we would like to get a um sort of flagship eval up and running relatively early to get buy in from the sector more largely but in the long term I think that this could probably be um we could use AI to leverage or leverage AI to improve these efforts. So the idea here would be to use the current models that we have available or available then to try to get at what it is that we're ultimately caring about and use that to steer the development of the next uh set of models and the next generation in a way that would um capture those values. Now I will say um to get this off the ground we would need funding and potentially also fiscal sponsorship. If you're interested please talk to me. I also see this as a collaborative effort that complements the really flourishing ecosystem of vivas that is available right now. So if you're in that ecosystem, please also talk to me. Talk to me regardless. I like talking to you. Thank you for having me. Thanks. See you all.",
    "char_count": 5353,
    "success": true
  },
  {
    "video_id": "_xtHcBQGYpE",
    "url": "https://www.youtube.com/watch?v=_xtHcBQGYpE",
    "transcript": "Thank you. And I will say I really enjoy the Sentinel newsletter. Wow, that's bright. Oh, good. We get to see my AI art again, though. So, um, all right. I think the manila filter helps some. And we're going to do before we start the time, we're going to do a quick adjustment there. I saw Olly running behind the screen. So, we're gonna see give it 10 seconds there to see if we can turn it down. I think everyone in this audience is excited enough about the future of potential AI powered strategic foresight to push through and uh blind yourself. There's some kind of pun there, but I'm not sure. I can't do it. It's too late in the day. Ah, the future's bright, but it's getting dimmer. All right. So, now I'm very excited. Woo! Uh, to welcome Alex and Emma up to talk about Wayark Labs and their initiative. One round of applause everyone please. >> Thank you. So um I came into this fellowship from background in policy government um and geopolitical forecasting pretty concerned about the capability of governments to keep up with kind of potential pace of change that we could see AI um as well as like other things um bring to the world. And so if we're thinking of some scenario where every government department in um every government around the world is having to really like rethink a lot of its plans um in some of the more optimistic cases um people like to say that oh well maybe AI will help us navigate better than we would um by default and we thought that it was time to start thinking seriously about what that would actually look like. Um so part of the reason is to get a bit of a running start on figuring out the form factors, figuring out institutional adoption. Um building that institutional muscle memory and getting it integrated into decision flows. Um but also because we think that LMS are getting to the point of being able to provide some real utility here. So um one indicator of that is forecasting performance. LMS are on track to maybe exceed super forecasted performance as soon as next year. um thing is we think are never going to be some kind of oracle telling us exactly how the future is going to go. There's going to be irreducible uncertainty. So we need to kind of take all of this um cognitive capability and integrate it into something that looks a bit more like um scenario planning or strategic foresight tools um to understand how the world might go grounded in as much as we can know about the future as possible um and then work out robust strategies on that basis. So to get started with that um we've been working on a design partnership with RAND specifically a team working on geopolitics of AGI and they're doing a lot of scenario work at the moment and running these tabletop exercises about what it would be like to be say in senior levels of US government and having to respond um to AGI disruption across a whole bunch of different policy areas. Um, and we've also been talking to the forecasting initiative because them and the gaming people have been wanting to bring their worlds together a bit and we think that between the three of us, um, there's a really exciting potential collaboration there. >> So, I'm going to make that a little bit more concrete with a demo. Um, one thing, I pre-recorded it. I cached the results so it doesn't actually run that fast, but everything you're going to see is all NLM analysis. Um the other thing is that it's pretty focused on RAM as a use case. We really like that because it grounds us and they're running a lot of these sort of exercises, but we think that the potential here is really more around like how do we get governments and institutions to have a lot more foresight and really think things through and the exact UX here is pro um so there's going to be iterations on that. So the situation we're going to set ourselves up for is um imagine it's 2029 GPT7 comes out and it has online learning capabilities and the idea is that you can provide a little scenario. We're going to define some metrics. We have some stakeholders for which we do some deep research and do profile analysis on them. You can provide some other upload some other data and then we for every stakeholder go through and say like what does their landscape look like? What actions should they reasonably take? We do a lot of analysis across many different factors. We come up with some proposals and eventually comes up with one action which we can override if we want to. And then the next phase is going to be then what happens and we do this in three phases. The first phase we look at every action and we decide can this stakeholder actually do this? Do they have the mandate? Is this technically feasible? Do they have the resources? And then we do a lot of analysis with LLMs to figure out like well let's consider this all the ramifications of what might happen as a result of these actions. And then the third phase is we take all those actions and then we see are there interaction effects um and if so what are the final predictions of what we think the world is going to look like afterwards. Now we'll note a lot of those problems look a lot like forecasting problems. So we take them aside and we do firmy estimations and we look at base rates and we say like well what could feasibly happen that could be very helpful because sometimes the uncertainty bar is like so large it might just make sense for you to branch off and say like well let's explore this other version and see what the world looks like then. It's a very flexible platform. We could add another stakeholder here. We add a Chinese government and we basically tell it go bully to the US. Be aggressive in any way that you can. And um yeah, it comes up with some strategies to like curtail and find places to basically impede the US's ability to make use of AI. Um or if we wanted to stay a little bit more nationally focused, we can add a uh like we want to look at like labor dynamics, we add a coalition of organized labor and then the system notes that because the people are like usually well educated in the white collar departments and they know how the systems work, they end up basically like causing um lots of protests. And if you follow this through the round to the very end, yeah, it ends up being able to come up with like a UBI system because it forces the government to take action. Then once we've done all these runs, we can have a three of them. There's a lot of work that we can do here, but this is just kind of gives you a bit of an idea or we look at each of the forecast numerics that were made and um yeah, just kind of like get a sense of how that goes. So there's a lot that um couple learnings that we had. I think my favorite here is that if you ask LLMs just to um think through second or third order effects, it tends to be very dramatic. Like it really thinks that like the whole world going to end just because like 30% of people lost their jobs. Um the solution there is to explicitly ask it for a forecast and to ground it and then it's analysis is pretty good. But that's an example of like why we can't just rely on chat bots and be like, \"Oh, what what happened?\" Like you really need to take the time to like get these LLMs to think things through a little bit better. um validation obviously very important. So here I've got a chart where we're going to check the internal coherence. So we run the same basically the same simulation a bunch of different times but on the x- axis I'm varying uh the size of a jobs program. So we're viring from like 100 million to 500 billion. And on the y- axis we see like well how many jobs does it actually create? And it's a log log plot. And we see well once we rerun the simulation every single time it has a nice diminishing returns. log log plot power law which yeah if you kind of look at the numbers this is very encouraging we think um validation is going to be a big part of the problem here which is why we think we need a little bit of lead time before we can really go into governments nicely a few think tanks are already interested in using tools like this rand being a big one we've talked to a few other researchers and they're just already starting to face this problem of like it's so hard to think through these problems that have like so many ramifications and we need better tools to help us like think through this. So, first step is going to be think tanks and then um there's probably some foresight units that we've had some conversations with that are interested but are probably going to be a little bit later adopters. Risk modeling as well is going to be a good use case for this. Um yeah, we think that this is a a very important problem that we think more people should be working on. We want to continue working on this. If there are people who are interested in using a tool like this, like come talk to us. Or if you might have connections to people who are doing this sort of stuff, like please come talk to us. If you have advice, anything like that, that'd be great. Um, and other than that, yeah, we need funding for the next year or so. Um, in part to fund our collaboration with Rand. Um, so if you have leads on that, we would love to talk as well.",
    "char_count": 9233,
    "success": true
  },
  {
    "video_id": "eHxQRoE3MmA",
    "url": "https://www.youtube.com/watch?v=eHxQRoE3MmA",
    "transcript": "Hi everyone. Um I'm Kai, a PhD student at Oxford. I spend a lot of my time in the fellowship working on generative agent-based kind of simulations. Um one of the use cases is pivotal which we'll be talking about right after this. The other is this one negotiation station which is a benchmark and possibly a training environment for negotiating AI agents. First why AI mediated negotiations? I realize I'm kind of preaching to the choir here. Um, but we're facing a lot of difficult, tough global challenges that will require us to coordinate and take collective action. Um, and we'll have to find new ways to do that. And there's some hope that AI will be one of the tools for this. That's why AI mediated negotiations have gotten attention from various different groups in academia, um, industry and even the public sector. However, as with all LM technologies, a big difficulty is benchmarking and kind of like building trust in these tools. Um, so that's where I think that uh benchmarks can come in. So I've kind of built a framework where you can have that in implements different kinds of AI agents. So both negotiators that have a position and want to reach a given goal and mediators that try to help them find a particular consensus. And you can kind of hook these up into various different architectures. And of course, this could basically work for any problems from like supplier supply negotiations to big international treaties. Um, but the thing that I've decided to use for the benchmark is cake cutting, which is a classic problem from the economic literature, where you have a cake with different kinds of toppings, different pieces, and agents with different different preferences that have to negotiate about how exactly they're going to divide up the cake between them. Um, so we can see that there's a lot of the structure that could go into more complicated negotiations here already. The way that this framework works is very similar to the way it works in pivotal um where we have like a mediator which generates an initial message. The participants then generate their replies. Um behind the scenes we have kind of tool util calls which check if there are suggestions andor kind of confirmations agreements um being reached. And finally um we erase all of the negotiating history from memory and ask the participants to rate the final outcome. um that that they got to quickly going to talk through a couple of the three key results here that we can kind of use to to simulate with this. The first to kind of get a sense of the breadth of possible outcomes is simulating different agent personalities. We can for instance tell the agents to be adversarial only care about their own utility and maximize the best outcome for them or to be cooperative and try to find the best joint solution. Um and what we see is that if we tell them to be cooperative they find solutions both more frequently and um faster than the other case. Second um we can kind of use the system to uh see what the impact is of introducing a mediator. Um and then secondly also perhaps by tuning the mediator itself um and that's what I did here. So if we just introduce a mediator the first kind of time around it actually slows down um this the agreement finding for adversarial agents. But now that we have the system, we can kind of iteratively improve the prompt. Um, which I did also just by feeding it into an LLM and asking it to feeding the in the whole transcript um of an of a negotiation and ask it to improve it. And so then eventually you reach can reach a prompt which helps the adversarial agents find an agreement almost as quickly as the cooperating agents. Finally, um, a fun thing is that we can also use this to compare the negotiating ability of different kinds of models. Um, so here looking at Claude Sonnet versus Claude Haiku. Um, and the difference is slight. It's still quite noisy, but you can see that there's a slight bias where the larger Claude Sonnet model is generating on average better outcomes. And yeah, there are still big error bars, but you can run this through a bunch of different um, companies and also see on average larger models tend to do better. Finally, going back to humans, we can imagine people entering into the system in different ways. First, we can replace the negotiating agents with um actual with people directly who interact with this mediator that we've benchmarked. And the second is that we can um we could also imagine people giving their preferences to an AI, one of the negotiating agents in the simulation that kind of like rolls out for them. Thanks so much. Um,",
    "char_count": 4631,
    "success": true
  },
  {
    "video_id": "jqss-3RYjaE",
    "url": "https://www.youtube.com/watch?v=jqss-3RYjaE",
    "transcript": "a mistake. [laughter] >> All right. All right. I love that they went before me so I don't have to spend time talking about what exactly community notes is. I can jump right into an example here. Um, so a couple weeks ago, Elon Musk posted about how a Tesla Cyber Truck can beat a Porsche 911 while towing a Porsche 911 on a quarter mile track. Uh, that's a pretty amazing claim. Uh, and it's there's parts of it that are wrong, although still pretty amazing. Um, it turns out that it was not done on a quarter mile track. It was done on an eighth of a mile track. Um, and in independent tests, the uh Porsche 911 on a/4 mile track did beat the uh Tesla, although the Tesla was still towing a Porsche 911. Um, but anyways, I didn't figure that out. I built an AI fact checker that figured that out. Uh, and then wrote a community note and then posted that. It got through the bridging algorithm. And, uh, this is currently showing live on Twitter. Um, hopefully I didn't make Elon Musk too mad and he's going to shut down the program. But uh one of the things I did that was a bit unique is that I built a system where every note that I submitted came with a link back to a companion website that had more details on it. So we can go and look at that here. So, uh the um uh the companion website has a breakdown of the claims and then further details and then also you can go and search across all the different posts that are uh in the system and I tag them by like political orientation and various other things. It's kind of fun to see the various types of like misinformation that's out there. So, feel free to check out the website if you'd like. All right. Uh so a a bit of details in terms of like how the system was actually working. Uh started with around um for for this particular demonstration I'm talking about a window of about uh 10 days uh in justested 2,000 notes. The blue line is kind of the happy path along the top. Uh and you can notice that it gets pretty small pretty quickly. Uh we went from 2,000 notes down to just 13 that successfully made it through uh the uh um the community notes algorithm and displayed. Uh why is that? Uh well first off um I excluded everything with video. video was just a difficult problem. So, put it off. That was about a third of notes. Also, a lot of notes are just really low clarity. Um, people request notes on things that like who have free time, DM me. I tried googling that. I could not figure out what what this person wanted for. Uh, so anyway, so I I built an LLM pipeline to uh to try to filter out those kind of like lowarity notes. Um, also, so we're down to about a third of the notes. Um, I'm actually not going to spend a ton of time. This was surprisingly one of the easier parts of the whole project. I thought I'd spend all my time on this. I spent most of my time actually on all the things around building the actual fact checker. Uh but a few details GPT5 uh average cost is a little less than 30 cents. Um took about five minutes to run. Um come talk to me later if you want to know more details about how I built it. Um I had a separate uh agent that was actually writing the the the community note itself. So there's the full fact check that goes on that companion website. Then there's the agent that tries to take that and make a 280 character uh submission. And also one thing that's key here is that um X really uh you know limits the number of submissions you get. And so uh you really need a process to try to figure out what are the the highest quality submissions out of all the possible submissions you can make. I cheated a bit here. I did this manually. Um it's totally automatable. I just never got around to that. Um but full disclosure, I did cheat in terms of like which notes got submitted. Um so going back to this uh my most successful community note, um that got 1.7 million uh impressions. Um and interestingly, what kind of traffic does the companion website get? So I instrumented that um with mix panel. So um not just that note, but across all the notes I had that got posted um about 2.3 million total impressions. Uh that generated just a little less than a thousand clicks to the open note network. Uh for 385 um dollars in LLM fees, excuse me. So um for the cost per thousand impression, this wound up being like really effective. It's a really cheap way to get impressions if what you care about is having a fact check show up on um X and then the the cost per click this is actually still fairly competitive like 39 cents on a on a click. Um so um you know with advertising you will get a certain type of person whereas with this it would be a particular you know people who want to go a little deeper. So it's unclear if you can really build a business around um instead of paying for advertising providing fact checks for traffic. Um but it might be possible. Um and then one thing just on the the limitations of uh the bridging algorithm I have to bring up um for really contentious issues. It just I don't think it works particular it's not very effective and I don't mean to like denigrate the community notes algorithm. Um I think it's uh amazing at what it does but a lot of the things that uh I managed to get through were kind of like not as consequential things. The the you know Republicans claiming that Democrats wanted to um give healthcare to illegal aliens. I looked at that nine different ways. It's just not true. But I'm pretty sure there are zero community notes to that effect. Um, and so I think there's a lot of work to be done in terms of trying to get at those deeper epistemic issues. >> All right. Thanks everyone. >> Thank you, Steve.",
    "char_count": 5672,
    "success": true
  },
  {
    "video_id": "lCqQIabLKVo",
    "url": "https://www.youtube.com/watch?v=lCqQIabLKVo",
    "transcript": "Okay, starting my timer. Okay. Hi everyone. My name is Sophia Vanhan or Sophie. I'm a computer scientist and software engineer. Um I spent this fellowship. I worked a little bit on cord or decision m and I also built and shipped uh this tool called magic search which basically matchmaking tech for live job markets keeping humans into loop. Um but now I'm going to be talking about uh future visions hub uh which is like this world building platform. It's a it's a working name. It's in the planning phase. It's this platform for creating sharing and sense making on future scenarios in world build. Um so here on this first slide we see we see AI 2027. We see the FLI world building competition. We see the foresight institute existential hope world building challenge and we see this upcoming fiction competition for center for safety. And what we're looking at here is a scene of valuable works of future thinking, scenarios, world builds, stories, really great valuable work. But what I also see is a lot of untapped potential. Um, you see these works are disconnected from each other. They're not into anything like more and further. The great ideas within these works, I think, do not end up in public discourse nearly as much as they should be. Um, a key part of this world work is to spread positive visions, existential hope, and enable better collective sense making about how we want the future to look like. But currently, none of this is properly facilitated. These outputs are all like basically walls of text where the key ideas easily get lost. If you only have 30 seconds to spend on any of them, you're going to get very little out of it. Feedback and further development of these ideas is difficult. For example, if you have a critique of AI 2027, maybe you can like blog about it or tweet about it, but most of the time it's unlikely that your contribution will be seen and considered by the creators. Um, you can email them, but then the broader community doesn't get to benefit from your critique. Um, this slide is about um my love for Weston and alignment forum which are epistemic infrastructure for rationality and alignment. And what I want to make is I want epistemic infrastructure futures. So, I'm imagining a platform that's optimized for creating and communicating and developing and critiquing future scenarios and predictions and ideas and fictions about futures. Um, I believe that all these works on the future could have a lot more impact if they were number one communicated in the right way and number two if they lived in the right environment among other similar works where other people could critique or work on them further. Uh for example, the right platform could enable like surfacing of the key ideas from the complex narratives that these problems are or identifying predictions and connecting them to existing prediction markets or they could help generate unique aesthetics or short stories to make scenarios feel like more immersive and believable. I also think the right features could enable better high quality collaborative decision- making about futures in particular resulting in better ideas like how less strong has resulted in now well-known ideas like the paper clip maximism. The broad theory of change here is something like we have this platform that makes features legible and modular and collects them in one place and enables discourse. Um which results in drastically increased progress on futures thinking um which then results in more and more and better memes about futures based on which reach broader discourse and then these memes actually serve as coordination tools. So coordination improves because we have the shared vocabulary and visions. So an example would be um an idea that I really liked in AI 2027 was how AI might start influencing politicians directly and the risks um related here and I think if voters journalists politicians in the in the broader world would know about these risks then they can look out for these they can look out for this phenomenon and they can mitigate it. Um another example would be like a policing policy proposal or technology. If it's like it was invented within this platform and they reach the outside world, people can start organizing in order to bring that idea to life. Um on the other hand, even if the ideas don't make it out of kind of circles, they can still have huge positive impact that way if we just have more and better means. Um it's like the plan very very simplified time reasons. Um but yeah, how I would move forward with this is um first to build build the MVP for like how to actually communicate worlds in like more immersive, more engaging, easier to understand and distributed way. Um how to automate that, how to bring all these words together like using this platform or using this um using the software to create to create these um immersive versions of existing world builds. then the MVP um of the sense making layer and then scaling up. This is I'm already kind of implementing this but it's so a simplified simplified version of the plan. Um and that's kind of it reard if you want to talk about future sense making or the future of the job market which was the magic search project that I mentioned. And here's my website again and thank you very much. I hope the hope the audio was okay.",
    "char_count": 5342,
    "success": true
  },
  {
    "video_id": "m5h8Sx8kx18",
    "url": "https://www.youtube.com/watch?v=m5h8Sx8kx18",
    "transcript": "Another round of applause uh for experiments in epistemic infrastructure. >> Awesome. Um we'll be covering the latter two. Uh we did build an epistemic arena tool. Um but we're not talking about that one today. If you're curious uh we can talk about it later. You can find us later. But uh hi, my name is Alysia >> and I'm Martin. >> Um and we're going to talk about some experiments uh that we did in epistemic struct infrastructure at scale during the fellowship. So both Martin and I are engineers, former YC founders. So we're very much more on the buildery side. We built a lot of different prototypes. We're going to talk about two different areas. Um today the first is the area of image epistemics and the second is riskatch which is a risks threat observatory enabled by prediction markets. >> So what is image epistemics as a nutshell? So image epistemics means that when you have an image on the internet, you know where that image came from. So where other websites it was posted and also what the context was. Um so we tried to apply this idea to write community notes on X. So we thought this was super simple. Uh a couple examples are breaking you know there's this disaster conflict happen happening and to change how you think because we made this claim and there's this photo that's very emotional uh that's potentially trying to change how you think. So sometimes a lot of times frankly uh you'll find that if you do a reverse image search you can find that that photo was actually made 10 years ago and is not actually recent. Uh so we saw those cases and thought, \"Oh, we can totally make a community note spots for this.\" Uh here's some specific examples. Uh the one in the middle is pretty much Oh, you can't actually really read it, so we'll just skip it actually. Um but what we were finding was that when we actually tried to use traditional reverse image uh engines, so things like Tennai, uh Yandex, Google image search and Bing, we found that those actually were only able to return an image that was found somewhere else on the internet around 1% of the time. Um so that's really really low, at least a lot lower than what we were expecting. Um so this brings us to an open question that we had. Why was this happening? And what does this mean? This effectively means that for 99% of these images, exact reverse image search is functionally dead. There is no way of doing reverse image search and we'll talk about why. There's a bunch of like conspiracy theoryesque stuff that we can talk about. I won't talk about in this presentation. Uh but in general, the nutshell is in late 2010s and 2020ish. Um even though the tech exists for this, this is not some impossible task in terms of technology. Um search just got so much worse. Hand wavy. Uh so why is it bad? Um in general, uh social media platforms are walled gardens which makes it very hard to crawl. Now if you use your social media, I'm sure everyone has, you'll see that image. Everyone postes it posts an image alongside with their tweet or whatever Instagram story they have. There's images and videos constantly. um and it's against turns the service to crawl it. So it's very hard to actually build a reverse image index that will have everything. Uh there's a bunch of other reasons here. Uh let's just move on for time. And so what we learned so that we learned that the real blocker isn't AI here. Uh the real issue is that there isn't a searchable commons that exists. So if you look at common crawl or other data sets which are effectively like 10 millions of images or billions of images, you'll find that even if you use reverse image search from those data sets and you cross reference it with the tweets, which is what we did, you'll find that a lot of the times you actually won't get a hit. And I think this is largely because these images are largely just social media based. And so our recommendations is that if social medias could effectively solve this themselves by building an an internal reverse image search and running it auton automatically on posts um so that they can get a clear case of recycled images or places where someone would want to write a community note. Um so looking at UJ at X that' be great to work on as an aside. Um so so another social another recommendation we had was that social media platforms could just provide a program where you know they can let's say uh sign some sort of NDA with a company where that company gets access to the images so that they can build better detection tools or some sort of agreement. So the second area uh is called riskwatch. So it's a a risk threats observatory enabled by prediction markets. So something that's pretty interesting is that prediction markets is pretty new. Um and I would say there's been a lot of collective intelligence that's been generated. When people use their own money to bet on some sort of prediction, they are essentially forecasting some result and as a result millions of these sort of forecasters have kind of gone online. But not a lot of infrastructure exists to kind of parse a lot of that intelligence and data. um and for a variety of areas that could be relevant to catastrophic risks. Um so we built a platform to aggregate predictions across poly market, manifold, kshi among others. Um and built a bunch of uh like filtering mechanisms like or uh and alerts for velocity of what's moving and why. um category deep dives for AI safety, predictions related to AI safety, biocurity, nuclear, climate um that can also help forecasters see related questions across platforms, get a baseline for um the kind of question that they're considering um and find some neglected opportunities or information gaps. Um in general, it pretty much looks like a coin market cap but for predict prediction markets. Um so there's some all events and they're they're ranked on um volume that goes through. There's different sections of military, geopolitical, biocurity, pandemic, AI safety related, nuclear, climate, environment. Um oh that's not loading, but you get kind of the idea. uh and some initial aggregate insights that we can overlay with a b with a lot of other information that would be useful in terms of adoption pathways. So um risk focus organizations so research in institutions that are monitoring these emerging threats um could be useful for policy makers and situation rooms to get real-time sort of intelligence for decision-m seeing what the general populace or crowdsource intelligence thinks about certain areas. um professional forecasters um education use cases, API access into institutional systems um and let's see what's okay um some open questions like are prediction markets actually good signals for all catastrophic risk types um some risk categories lack market depth and liquidity the space is pretty early but not a lot of tools exist for this sort of thing um I'll probably skip through because I see Tim has stood up um but in general we've been drawn to problems where volume overwhelms human capacity to process this information whether it's through image epistemics or um in creating these tools for in aggregating collective intelligence via these new kind of information flows that have come online through prediction markets. So yeah that's it. Um, if you're curious about other uh experience that we made, feel free to talk about talk to us.",
    "char_count": 7382,
    "success": true
  },
  {
    "video_id": "qkvFfS_nTI8",
    "url": "https://www.youtube.com/watch?v=qkvFfS_nTI8",
    "transcript": "Thanks. Thanks, Ben. Thanks, everyone. Um, uh, maybe I'll just give a tiny bit of context. I was trying to decide what to cover here. First of all, I want to apologize that my slides are very simple. I've been using LLM so much that I forgot how to use images and everything like this. Everything is text. Um, I'll work on that for a future version. Um, I was reflecting on whether to talk a little bit more philosophically or just really practically because it's this has been a really exciting period just because I get to talk to people who are very hands-on builders and some people who are incredibly philosophical and I try to do a little of both, but this has been an interesting experience of just exploring a few few areas. I'm going to go a little more philosophical, but if time is short, I would love to show anyone demos or hands-on stuff oneonone or or in person or chat about it. Um, my own background, I'm I've worked as a like very hands-on founding engineer, but I've also done some research in the past, research labs, and I this feel like this has been a kind of a a period to be exploring software engineering and knowledge tools and then in particular around some of these topics around eval. So, I wanted to give a little bit of background on like my interest because I think there's many ways to come at these problems. My interest is I really am interested in human reasoning and how do we use AI in a way that's um that's very effective at making us smarter at solving problems and we could hopefully use AI to make ourselves a little smarter a little bit along the lines of Doug Angelart's old vision for those who know him. Um so we're incre So I just wanted to talk about this because some people I think are a little hesitant. Should we make agents smarter at all? I would say broadly yes because if we have smarter agents we're smarter too and we have a lot of problems we need to solve and we need more smarts to do that. So that's my take, but that's a discussion to have. It seems like one of the two things that most help is if we have agents or LLMs had more vigor and more groundedness because we know they tend to um there's some problems uh with agents. They we often don't know why they tell us answers or like LLMs why they give us answers they do and they're often very confidently wrong and I think we're all very frustrated with that. So I had been experimenting before fellowship and in the beginning part of the fellowship of doing just fact-checking like take documents take things written by LLMs and try to do web searches check them much like the sorts of things like the community notes efforts earlier and this is really interesting and really valuable but I found it kind of frustrating as we began exploring because you're still at the end you're like okay now I'm checking what the LLM checker said and do you trust that and it's like turtles all the way down so it's hard it's slow and it's also kind of sparse data so I ended up thinking more about this and also talking with a friend and we came into a somewhat different direction but if agents are being used for more and more tasks um one angle and I didn't predict this would happen ironically but I found I getting much more interested in predictions like a lot of people here and if you can make predictions and check them that's like a real groundedness check of whether an agent is doing something legitimate and it's much different than trying to evaluate it by just checking sources that you might or might not trust as well so that's why predictions seem really powerful Um, agent prediction strategies uh are sort of like what's behind the scenes of these more complicated orchestrations of LLMs that we're now working with. And I just mean agents in a very broad sense. I think there's a lot of terminology, but like it's just this like orchestrating things to do a goal, maybe make a prediction. But if we could grade agent predictions against reality, we get a lot more benefits. And we're seeing that with a lot of reinforcement learning environments that people are talking about, like for robots or for things like this. But how would we do this in a more effective way? And it's just something that been exploring and thinking about with different folks. And this is a somewhat new area for me. So if anyone has feedback or thoughts on things I should be talking about, please fill fill me in. But it seems like there's three things we really care about. We care about observability to know whether an agent is really why it's suggesting or predicting the thing it is. Groundedness, that it's really tied to reality and not just stuff it found on the internet and the training data. And that it's scalable. This is the third part that I think is kind of different and a little bit different with what we've been looking at. Um, is that if we make predictions but we have to wait six months to find out whether we were right about who wins the election. That's like waiting six months for one bit of information. And did were we right by luck or skill? It's very hard to know. But if we could scale the information, we would learn a lot more. And that's what's gotten me very interested in the idea of cheap gradability of agents. So if an agent is um uh you could grade it very quickly. So, and then to test it, I'm going to go really fast because this is a lot to cover, but we started testing it as a test bed with financial predictions because financial predictions are the one place you can get grounded APIs to historical data. So we have built I'll skip this but this is kind of like thinking about this but I built we've built a a sort of framework for running these agents and I'd be glad to show anyone who's interested where you could do back testing of predictions at scale and kind of see whether agents are accurate or not and then perhaps explore what reasoning strategies make agents more effective at really predicting facts starting with things like APIs like finance but it could be other things as well. So that's the broad idea. I could try to demo it, but I will not. I'd be glad to in person uh if anyone was interested.",
    "char_count": 6169,
    "success": true
  },
  {
    "video_id": "r_vdUeoKbJE",
    "url": "https://www.youtube.com/watch?v=r_vdUeoKbJE",
    "transcript": "Hey, how's this? >> Yep, we can hear you. >> We coming through. All right. Okay. So, um I'm Gordon and sorry I couldn't be here with y'all today. Um stuck in Nairobi due to a visa mixup. This is just one of the nonlinear uh effects that we feel on a day-to-day basis. I've been building Deep Future. So, Deep Future is an AI agent for scenario planning. You could think of it as deep research uh but for strategic foresight. We start scenario plane with a focal question. So I'm actually going to go ahead and ask Deep Future, how will AI agents transform the web by 2030? This is something I've been interested in as a founder. So we'll hop in and Deep Future uh drops us into a new scenario planning project. So scenario planning is a structured research process that was developed at RAND during the Cold War and it's used extensively by the US military and other orgs that need to make high stakes decisions uh to be able to basically evaluate your strategic environment and create strategies that are robust across a wide range of potential outcomes. So you can see Deep Futur is doing a lot here. Um, it's basically updating its memory, reading through a handful of prompts, but it's asking me some follow-up questions, helping me scope my question. And defining a good focal question is key for getting good results. So, it's asking me about specific uncertainties, my perspective, and role. I'm going to go ahead and say I'm a founder looking for startup opportunities, inconsumer platforms. I'm concerned about big tech companies aggregating the space before I get there. We'll see what it does with that. So uh one of the things we've done with deep future is develop a fairly sophisticated memory system to allow it to manage this scenario process which has a lot of different research phases. So we needed an agent that can manage longer task horizons and uh we've got this memory system that helps us do that. Okay. Okay. So, it's got a handful of questions for me. So, I can further refine my focal question, but it's got some pretty good options for me here. I'm going to say let's go with option A. Let's go with option A. And we should see the agent update our project focus and move us along to the next phase of our research. All right. I love how my life is just little 10-second breaks between inference. Now, so in this next phase of scenario research, we identify the forces that are driving change in our environment that are related to our focal question. As Alex mentioned, getting good forecasting or foresight results out of these models is all about grounding. So we want a structured method to be able to analyze the environment and analyzing forces is one way to do that. So I'm going to go ahead and say uh can you identify three forces uh related to economics in this space. We'll see what it does. So along with the memory system, we've given the agent a handful of tools to assist it with scenario planning research as well as the ability to contribute to a scenario database that basically allows us to build up a kind of model like a world model of our problem space. So we can see it's identified AI infrastructure cost collapse. That's a pretty good one. Big tech acquisition spree. Kind of interesting. Maybe a little uh vanilla there. venture capital as a driving force. These are all pretty good for each of these. It estimates a level of impact for our question, a level of certainty. Um, we can go in and edit any of these things, but it generally does a pretty good job. Now, uh, I thought I might open it up to the room and ask, is there anyone who can think of forces that are driving change for this question? How AI agents are going to transform the web. We'll see if this works. I don't know if I have an AV link. We're gonna give this a try. >> See, Kathleen, I'll give it a few seconds. >> So, audience participation. Uh, all right. I'm going to run the mic over. >> All right. I'm I'm not hearing anything. I'm sorry. So, I'm gonna go ahead and Okay. Back up. How's that? Okay, >> the so the prompt is um how will AI agents transform the web by 2030 and we're looking for any big drivers like technical breakthroughs, political changes, regulator regulatory uh economic drivers like uh concentration among monopolists, things like that that are going to have long-term impact. >> Okay. So, >> the invasion of Taiwan, >> that's a really good one. Let's add invasion of Taiwan. All right. Yeah, that's a really uh good political driver. Let's see if it actually registers that as a political driver. Um maybe a social driver. Uh so this structured research process, we basically go back and forth. The AI can identify a lot on its own, but generally it's nice to also be able to be in the loop. Um so it gives it a, you know, reasonably low probability in the next uh five years, but an extremely high impact. I'd probably bump up that probability myself. um bump up our certainty about the outcome. A little bit scary, a little bit pessimistic. Um so once we've identified enough forces, I'm going to actually hop over to uh a previous playth through on the same question. So we've got 38 that the AI has helped me identify. I did most of these. I added a few myself. Deep Future actually hops over to the next phase where it helps us identify structural connections between these forces. So taking a look at some of these We've got uh an AI lit movement and that AI lit movement is connected to white collar knowledge work displacement, AI addiction, synthetic media trust collapse due to slop. Moving over to the political realm, we have AI model supply chain politicization, US China export chip uh control uh chip control export laws, geopolitical energy partnerships like with the UAE, China's domestic AI chip push. So, we're already sort of seeing how there are these bundles of forces that play off of each other to create nonlinear effects. And once we've connected all of the forces, Deep Future helps us map them on impact and uncertainty. So in the upper left here, we have high impact, high certainty forces like energy being an AI scaling bottleneck. This is going to be like a force that drives things. It's kind of a backbone in almost any scenario. Uh over to the upper right, we have our wild cards or black swans where plot twists come from. For example, AIdriven nuclear power renaissance. uh maybe not so certain that the US and Europe will reform their their permitting and strategy around that but if they did it have a profound impact for our question likewise AI bubble not maybe a big deal if you're thinking longer time horizons AGI but for a short time horizon and for our question it could make new winners and losers so after we've connected everything mapped it out and we start to get a sense of the possible scenarios deep future will actually generate us a strategic report including a highle summary summary, some key insights and patterns by theme, some strategic implications for our question, a table breaking down opportunities, allies, threats, and competitors, some critical uncertainties and signpost to watch. So, you could think of these as early warning signals, and finally, a sort of um a set of strategic recommendations broken down by time horizon that we can carry out, moving us from observation to action. Uh, now this is really early. Um, I'm already finding it really useful, but we're planning on doing quite a bit more, expanding deep future to include the full scope of a scenario workshop. This is something that usually takes about a week. We plan to compress it to 10 or 15 minutes in YOLO mode. And we're also going to introduce automated AI horizon scanning. So basically scraping news sources, identifying driving forces, updating models, and triggering early warning systems so you get a notification and you can act before it's too late. I'm really excited for the applications here both in in X-Risk and in many other areas. Um, so that's Deep Future. Uh, if you're interested in funding this work, uh, you can reach out to me at contact deep future.now. That's contact deep future.now. now. And um also if you want to be an alpha tester, please uh reach out as well. Thank you. >> Thank you, Gordon.",
    "char_count": 8218,
    "success": true
  },
  {
    "video_id": "uX3EdKWo3ZA",
    "url": "https://www.youtube.com/watch?v=uX3EdKWo3ZA",
    "transcript": "It is. Hi guys. I'm Matt Brooks. Um I'm a founder, a tech founder of an HR tech company. So I'm a builder, a coder. So I think through building. So I did a lot of MVPs during the fellowship. A lot of just iteration. And I'm going to go through a few of them and tell you what I learned. Um so the first thing I built was an offers and asks Slackbot. Uh you're thrown into this fellowship of 30 people. You don't really know them. You don't know their skills, their history, their context. and you might have topics that you want to talk to them about or or get some feedback. Um, so I built a Slack where you could just put in a query and it will match you with fellows that have the skills or they're talking about it in Slack and it worked really well. Um, it would match very accurately. Uh, the bad part is no one would remember to use it. It's not really in your like flow of your work. You're not really thinking about it. You forget the app exists. So something to think there a learning you could take away is that you need to meet people where they are. you need to reduce the friction and AI is actually great at that because it can play as that like input output um custom layer. Um although Slack is also a good um frame to put things in. So it worked great but it didn't um kind of stick. The second thing I built was Matt GPT digital twin. Fed it all of my preferences and text messages and Slack messages, all this stuff. And on the topics I gave it data on it actually worked really well. I think it did emulate my kind of how I think, what I care about, um, how I talk sometimes very in a creepy way. Um, you can get like 80% of your preferences easily, the bad part. Um, the important part about what you care, your worldview, but like 20% of you is like in your head and it's so nuanced and niche and it's not really in your even in your text messages, it's not in your emails, it's in your head and it it'll be really hard to extract that. So what you need to think about this is uh for your use cases of AI do you have the data that you really need do you have um the data like online digital can you extract it or do you need to create it or is it locked in someone's head and that's going to be a bottleneck third thing I built sealed.com that's live it's a paro improvement to admononymous if you've heard of that it's a anonymous suggestion box the idea is you use an AI as a filter so people can't be mean to you or send you like infohazards or something Um, so it it actually works. It's it's great. It's very easy. It can be a good filter and you can customize your filter. The bad part is I only solved one of the bottlenecks. Um, the bottleneck of maybe you're nervous that people will be mean to you. Maybe also the bottlenecks are you don't get enough feedback in general, so it's not worth it or you don't trust the giver because you don't know who the the feedback giver is. And AI can't really solve that. And so I solved one of the bottlenecks. That's kind of cool, but uh it's still not maybe not like the breakout product because there's other um bottlenecks. So you have to think through like AI can make a product better, but like is that solving the core issue that's making the product limited? And the final thing I'll talk about is um EA global matcher I built. Uh this was the best one by far. I'm going to say that I found product market fit in 30 minutes. And uh it's because it was the perfect storm. The data was there. All attendees fill out a pretty detailed form about their skills, expertise, their um their background, and also what they're trying to get out of the conference, why they're at the conference. So, it's their offers and their asks already presented in a data set. There's a thousand people that come to the conference and um you're supposed to book a lot of one-on-one calls. That's or the meetings because that's the most kind of, you know, in-person touchyfey like get into the deep discourse. Um the problem is a thousand people. How do you find the 10 you want to talk to? And so, I built an AI thing that took all the data and it it it told you the 10 that you should most talk to based on what you were looking for and what they were offering. And it was perfect because all of the data was already there. I just scraped it. I downloaded from a from a Google sheet. It took no extra time, effort, or money um from participants because I already did all the analysis, the scraping. I just spit out your matches. You got it for free. Um all upside, no risks. They did not have to meet with these people. It was just suggestions. Here are 10 people that fit the criteria that we think that you would like, but you can just uh do nothing if you want. And you could also very easily dive deeper by clicking the swap card and be like, \"This looks interesting. I'll spend three more minutes reading their profile.\" And so you get this like opt-in opt out thing. Um it was it was great. Uh the only thing was distribution was limited. I just posted it on the Slack. People ask me, oh find me my matches and I did it. But uh something to think about is distribution is always a limiting factor. You need to think about how you're going to reach all the users. Obviously the learning here is I should have done it earlier and then partnered with CA to try to like get it out there and get a little bit more formal. Um, but yeah, I think people loved it. And I think the takeaway on this last one is that if you have the perfect storm of like the data and the only bottleneck, the main bottleneck is you just need like cheap intelligence to throw at a thousand data points, AI is the perfect bottleneck solver and um, you can get product market fit in 24 hours. Thank you.",
    "char_count": 5736,
    "success": true
  },
  {
    "video_id": "vqDRlSWTOUQ",
    "url": "https://www.youtube.com/watch?v=vqDRlSWTOUQ",
    "transcript": "Uh so we did AI discourse sensemaking. Um we did kind of half and half. I did in-group analysis of mostly um in-group Twitter and Nikki mostly did outgroup analysis um which was true social and blue sky. So I'll start with Twitter. Um Twitter plus EA uh EA forum less wrong substack. We could say that's in group. Um Oh no, that is not my presentation. Don't look at it. >> See, this would have been a great moment for me to do that announcement about the sun in the shade during this part. Oh my gosh. Great. >> There we go. Okay. Uh so ingroup is apparently EA forum less wrong Twitter substack. Um and that is ingroup. Maybe you'll recognize some faces if you see the high quality definition enough. So if you remember hive one this like leaderboard ranking authority leadboard thing. It was kind of cool. Um I just like rebuilt it for AI discourse. Um, so the bigger, oh no, the bigger um, bubbles are higher authority accounts. Then I made it into a formal leaderboard and did another layer of analysis on the content of their Twitter account and what we could find about them online as well. And you probably know these people. It's kind of cool. Uh, then I did that for orgs as well. I like to point out that thinking machines and nurups beat meta. That's kind of funny. Um, and then once you have all this data, you have like hundreds of thousands of tweets, you can start doing some sort of like automated structured analysis. And so what I did was extracted five top clusters. These shouldn't be surprising to you. You know them, but it's cool that it just fell out of the data. So pragmatic safety establishment, the high doomers, frontier capability maximizers, the open source democ democratizers, and the paradigm skeptics. Um I won't read it all but uh it's interesting that these are types of people that you know or organizations that work uh you know in the ecosystem or content that you read. It probably is related to one or two of these clusters. Uh what else did I do? I turned one of the clusters into a chat app because why shouldn't I make a digital twin of a pdmer? So you can go to pdmer.versal.app and debate with a high ipdoom agent. It's pretty good. It's it's from uh yeah Twitter and um Substack and uh forum extracted data. You also get other leaderboards like domains um obviously have like 10,000 domains but just point out some cool ones. ZV is very popular above openai.com. That is crazy. AI 2027 is top 10. Joe Carmith is 11. Lucas is 17th. Manifund 21. Fore 25 above cloud AAI. Good job guys. This is this is impressive. Um, then I did a bit of a deeper dive on the higher quality detailed AI trajectory pieces of content. Probably recognize at least most of them. Again, just using AI to extract and distill it, found top drivers, technological, economic, organizational, governance, safety, geopolitical, societal. You can map that out and get the, you know, the summaries and the cruxes. You can find disagreements. um you can find recommendations and it's just cool that if you bring in millions and millions of uh words, you can just have AI labor uh read it all for you and give you the most interesting kind of takeaways. And so that was the seemingly like highquality in-group sort of discourse and content. I'll pass it over to Nikki to talk about Blue Sky and True Social. Thanks, Matt. Um, so, uh, Matt and I pulled, um, Matt and I pulled a bunch of data from, uh, Blue Sky and Truth Social in the month of September, um, with the goal of like looking at where the left and right agree and what the discourse looks like and just getting a rough sense of, uh, how people are currently uh, talking about um, AI online. Um, I think Blue Sky and Truth Social are nice uh self-contained examples because they roughly similar users and they've both left Twitter for very similar reasons and that they uh feel uh politically isolated and have sought a platform of their own. And so we kind of used Blue Sky as a standin for the left and social as a stand-in for the right and uh had a look at uh and built an automated pipeline to uh analyze uh what they're talking about. Um, so here's a uh map of the pipeline. Unfortunately, we have a lot more blue sky data than we have truth social data. This is because Blue Sky is an open platform uh and Truth Social is not. And so Matt uh painstakingly scraped uh Truth Social whereas uh I just had a um script to very easily download from the Blue Sky API. Um but um filtered them down to uh 10 uh 10k posts from both platforms and then went through a process of uh identifying where the platforms uh users agree and where they disagree. Um so first you have a lot of posts uh and uh just ran a first pass filter to try and identify uh what content is worth engaging with and uh looking at. Um so I ran a so I embedded everything uh and then ran a clustering on those embeddings and then had GBD5 label all of the clusters and identify the ones that are worth um worth looking at. Um then uh once we got down like once once we got down to uh the 10k posts from either platform uh I ran a uh had GBD5 again read all of them uh pull out uh all of the claims and then also uh rate each one in terms of how their like sentiment on AI as a technology. Um this pulled out roughly uh uh 34,000 claims. Um, so a real quick look at the sentiment. Um, most of the sentiment is pretty negative. People don't like AI. Uh, and they all have very different things to say about AI, but uh, the the statements are mostly negative. Um, one thing to point out here real quick is that Blue Sky has mostly like made up its mind about AI in a lot of ways, whereas Truth Social hasn't. The content on Truth Social is much more mixed uh, than the content on Blue Sky. Part of that is like an artifact possibly of the fact that the blue sky data was filtered much more heavily from an initial uh initially larger set of posts uh than truth social. So this might not be super robust. Um would have to do do more on that in the future. Um so originally I was really hoping to do some more advanced things with the claims. Uh but unfortunately looking into them a lot of the claims were kind of iffy. So, one thing that was really annoying is that the model would very often pull the opposite claim out of a post if it didn't have enough context on it. So, if people were sarcastic or they were offering like sort of I don't know like funny commentary on uh so for example, I got really excited at some point because I saw a claim talking about support for AI rights on blue sky and I was like whoa that's really interesting. I want to learn more about this. And it was just people dunking on someone who was talking about AI rights. Um, and so there's a lot of that. So it's a little difficult to get really useful things out of the claims. One thing that the claims are really useful though for is normalizing for tone and style. Uh so I want to compare I want to see like where the blue sky posts and the truth social posts overlap and doing things in claim space was really nice for um uh making sure that uh we actually like are able to like form these clusters with a nice even mix of um blue sky and uh truth social posts. Um so I ran a similar clustering to the clustering I used for filtering. uh embedded uh now in in claim space, embedded the embedded the posts and then pulled out the clusters that had the most like uh engagement from both platforms where the engagement was like uh you know even um and then uh from those I had uh GBD5 read all of the posts and then pull out uh for each cluster a set of things that people agreed on uh and a set of things that people disagreed on. Um uh there's also I don't know best way to share this but I I have a link where you could go through them all if you want. Um and uh they come with citations of uh the posts that they reference and I had a look through them and 80% 90% actually the citation supports the thing it's claiming. Sometimes it's a little so you have to you have to you should check the citations. Um uh so some quick highle takeaways. Uh blue sky is much more consistent. They really don't like AI. uh and they've they've decided to social is much more mixed. Uh the things come all over the place. Um uh and yeah, most content is negative. Um so first thing on the disagreement, Blue Sky uh doesn't take AI capabilities very seriously. Uh treats social is much more open to it. Um Blue Sky really just like doesn't want to I don't have that much time. Let me skip through this really quickly. Some agreement they really hate big tech. They're both scared of an AI surveillance state. This comes up a bunch. Uh, and this is all automated. You can repeat it as many times as you want. I think it would be really cool to do something where you track the changes in public opinion. So, if you just like run this consistently and you see trends change, I think that could be potentially really useful. Uh, you could do this for other media. Um, and uh, all of this I think could be really useful for trying to prevent uh, polarization. I think polarization could really kill any hope of uh um any like real policy in the future and uh I think activists could do more to prevent that.",
    "char_count": 9215,
    "success": true
  },
  {
    "video_id": "xsvCYhcxDX4",
    "url": "https://www.youtube.com/watch?v=xsvCYhcxDX4",
    "transcript": "you. Hey everyone, my name is Jamie Joyce and when I was in college, I thought I would make a great CIA researcher and I met with the former director of clandestine operations during the Iran Contra affair and he agreed. Um, and then I looked into the Iran Contra affair and decided that's not what I wanted to be a part of. Um, but then I also read the works of Benjamin Franklin and thought, hell yes, I want to enable scaled truth seeeking and improve intelligence so better democratic governance is possible. So, I've been working on that within a nonprofit organization and we are called the Society Library. Oh, I can do it. Um, maybe thank you. Um, and we have been working on improving epistemics with AI for years. But before the LLM boom, our epistemic stack was a collection of manual methods and processes, some of which are actually borrowed from CIA tradecraft. But now we are automating and scaling all of this for AI or with AI for public benefit. And this includes systems which search for evidence agentically extract arguments and claims for multimmodal artifacts, fact-checking and steelmaning those arguments, searching for chain of provenence, building indexes uh and also the logical deconstruction of artifacts. And together those tools uh have met the intelligence needs of news organizations, LLM companies, government officials, researchers, fact checkers, nonprofits, and members of the general public with outputs like high-quality reports, AI governance policies, drafting legislation, building decision-making models, scaling, fact-checking, building grounded data sets, and making deliberation graphs. I know this is a lot of information. Um, so we build many of these tools uh so that they can be combined in different ways to serve different needs. And we also have an ongoing collaboration with the internet archive to get more government data online and accessible. Good epistemics require good data, but only 3.6% of declassified and unclassified federal government knowledge has ever been digitized and therefore it's not been seen by an LLM. And only 2 to 5% of what uh the government digitally produces across agencies in terms of data even gets saved. So we work on getting more data accessible to everyone. Uh we do a lot, but this is not an organization that's just doing a bunch of random discordant things. All of our tools and work actually integrate into our ultimate goal, which is setting a new standard for what it means to earnestly seek truth while also enabling the capability to do so by building tools. Uh, and we believe maximalist truth seeeking is a critical foundation that is largely neglected in earnest. Um our effort to bring about these standards and capabilities culminates in what we're best known for which is structured deliberation graphs which logically model the complex reasoning from different points of view including a chain of provenence and Socratic style deconstruction the output uh the output output of which uh we think can be a benchmark for evaluing systems. Uh but really quickly what are these graphs today? You've seen some polling and clustering tools which answer the question what do people think and prediction market tools which help answer will such and such an event happen and why. Uh but deliberation graphs are used to answer a flexible set of other questions like what should we do to stop X? Uh what should we do about X? What is the crux of X issue? What should be the guiding virtue in X issue? Etc. and actually seeking the true or optimal or consensus answer in a western logical modality requires not only formalizing arguments but steelmanning them and then deliberating it in a full context and we believe that processes that fail to go this distance are basically cherrypicking and this is the natural end to fact-checking which is full deliberation checking uh arguably even bay theorem can rely on arbitrary pri priors if not maximally accommodating the observable space so our posture is that this maximalist deliberative context should be the minimum standard for truth seeeking in a traditional western logical epistemological modality though of course we're exploring other epistemological modalities especially in the coming year. Uh the persistent problem we found is that people chronically overestimate what they know or believe uh is relevant due to cognitive biases and it's actually pretty hard to get around this unless people do the diligent work of having their minds absolutely blown and humbled by doing the kind of uh mapping that we're talking about. And uh while we think our graphs may be useful benchmarks which I can talk about more later because sadly I only have five minutes. Um these graphs are not the output. They simply inform the standard of output. And so in this fellowship we tinkered with a number of projects. But most impressively we worked on automating a pipeline which produced a grounded fact checked 600page fully cited collective intelligence report on a complex government event. We pulled arguments and evidence from over 5360 sources across 424 domains. This includes YouTube, government websites, news and court documents. Every single claim has its own report about veracity which we are still painstakingly QAing literally every line of. Uh why do this? We were asked to and in general we believe that public and institutions should have excellent intelligence sources if they want to without needing the expertise nor needing to spend thousands of research hours piecing together and factchecking claims. A library of quality multi-perspective knowledge should just exist. And uh we need to move faster uh full automation. Um politics moves fast. I know you're coming to get me. Politics moves fast and there's an urgent need to keep up. Um, we were given money to uh distribute for other tools like timelines and analyzing video and modeling stakeholders. So, we hope that you will consider partnering with us and distributing funds so that we can build more collective intelligence tools for everyone. And thanks.",
    "char_count": 6072,
    "success": true
  },
  {
    "video_id": "yqpcsat1Mxw",
    "url": "https://www.youtube.com/watch?v=yqpcsat1Mxw",
    "transcript": "Great. Thanks very much folks. Uh Rob and I care a lot about people coming to answers they later endorse. So helping people come to answers they will later think were answers were good answers. These are some of the projects we've done. Each is a separate project. Too many projects. Um so in February I thought about community notes. Could we maybe forecast which things would get community notes? Uh and I started bothering Jay. Uh and that'll come back. So anyway, what is a community note? Well, here's an example of a tweet about 9/11 saying, \"Ah, they didn't have airphone planes.\" Uh, sorry, airplane phones. Uh, but they did. And so, this is a community note. It will sit beside the tweet. Everywhere the tweet is, the community note will be saying, \"In fact, airplanes did have phones in 2001.\" Or another one from the very trustworthy Illuminati bot saying that they know something more effective than chemotherapy. I doubt it, and so do community notes. uh moderation and factchecking have been sort of slowing down recently. Uh you know Meta has been backing away um but community notes are very popular. So Meta are trying community notes. Obviously they're an X but Tik Tok are as well. So this is something which is sort of growing in the facteing space. Humans write them, humans vote on them and then X displays them according to an algorithm that I'm not going to talk about right now but it's very cool and they're effective. Community notes reduce the chance that a misleading tweet is shared perhaps like 25 to 50%. But it really matters whether you get it early. So this line is like if the community note happens within the first 12 hours you see like big impact. But if it happens after two days almost nothing and notewriters are past their peak. So this is like the number of note writers on X. There's a peak here but it started to fall off which is a problem if you want lots more notes. So normally as we say humans write them, humans vote on them, X displays them. But recently AIs write them, humans vote on them, X displays them. So this is still humans voting, still X displaying based on human votes, but now there's more pipeline. And this is better because or this is this is very useful at least because AIs can be scaled very far. It's very like cheap to scale them far cheaper than humans. They can produce notes earlier on in the cycle. If this is like a tweets sort of cycle where it first of all gets shared and there's like a a long trough of views per minute then we can maybe get earlier to that initial peak and it produces notes perhaps on specified topics that have been written. So like if this is a human note which like cuts out this many misleading tweet views then maybe this is like an AI one which hits a bit earlier and ideally you'd like hit like as far close to this peak as possible really getting this like big initial uh peak of views. So we built an AI notew writer uh they're all named birds that's that's how that works. It was called bird watch. So they all have a gre it's it's a type of bird and this is the repo. And so uh our notew writer wrote the world's first ever AI written community note that was approved helpful. This is about football. Uh there are supposedly only three teams that have never been relegated from the Premier League. Wrong. There are six teams and our notewriter notes that. You'll be very glad. Yeah. This is and and Jay notes it as well. This is a trustworthy source such as you could put in a community note that that's the case. Uh so there's like a number of different kinds of these. So like this is like a sports one. Uh this is one where we point out that sometimes polar bears do walk on land. People thought polar bears were only on ice. That's not true. Uh this is one saying the British government will not install things on your phone without asking it to. They they will not in this case. That's that's that's the case. Uh this isn't actually one of ours, but this was the the really terrible Michigan attack, but this isn't a photo from the Michigan attack. This is from uh I think is it Ethiopia? Maybe this is from somewhere else. And and this was quite impressive. I don't know how they did it, but they reverse image search and they said like this isn't actually an image from the thing. And I think we're going to see more of that as AI becomes more prevalent is where is this image actually from? So our notes have been viewed about two and a half million times already. Um, and it's worth thinking there's like maybe an extra million views therefore where tweets didn't get shared where they would have done and that's pretty good too in my opinion. Uh, so what have we done in the fellowship? Uh, we've built some bots. We've made it through X's pipeline. We've iterated a lot. We've built filters to stop bad notes with different levels of success. And we built a lot of tools to try and like you're constantly iterating on the notwriter and you're trying to decide whether it is in fact better than the previous version of the notwriter and that has lots of things I could discuss. Uh and now we're starting on like an agent that calls its own tools rather than just being a specific pipeline every time. And now I'm going to pass on to Rob. All right. So I'm just going to cover a few other things. Uh the one thing we want to say and this is totally tangential but a lot of our work in AI forecasting actually overlaps with our work in writing community notes and that's something that will probably continue going forward. So like gathering news sources, vetting sources, extracting claims, all of that stuff is like useful in both domains. Um what's like on the horizon for us? Like Nathan said, we're going to be working with more like agentic flows that like do tool calling. Um we're really interested in video notes because we think that that will solve a lot of problems and it's actually kind of tricky to to fact check videos um technically speaking and yeah oh the biggest not the biggest thing but a very large thing we want to do is try to bring community notes to other platforms. We think that this is like a really useful thing that we need to push into other spaces. Um, and then we have just some kind of like northstar metrics for us. Tens, hundreds of millions of note views on X, nonX, note views, uh, video views, and ultimately, yeah, Tik Tok, Chrome, Perplexity, YouTube are other places that we're interested in going. Uh, so we'd like to keep working on this and I guess that'll cost, I don't know, somewhere between 150K on like a part-time basis or maybe 500K if we manage to employ additional staff. And I guess we'd do something between more note better notewriting tools uh to you know lobbying organizations. I think organizations are very lobby lobbyable. I think X just you can just DM them. Just just bother Jay until he does stuff. I think they were going to do this anyway. I'm not claiming we really did it but I did bother Jay a lot. Uh so and I you know I can bother I can bother anybody in SF. You know I'm I'm botherable bothering anyway. Uh you know I just I think it's underrated. That's my big tip. You can bother tech companies. Um so anyway, why us? And and my theory here is that we've tested a range of hypotheses. Uh I back us to test hypotheses and deliver some value on these kinds of of things. And so here are some things we tested and whether they worked or not. This was a tool to like show different people's opinions on AI and did people like it? Not really. Uh here's another tool we built to like express probability uh to link. So this like properties like flowing through these nodes and it was kind of neat but it was like hard to really get people in interested in it like we we did a lot of yeah it was mediocre bird flu risk.com I think was really good didn't get many shares which I think is a good thing but people were quite interested in it and in the disaster case where bird flu had been a problem I think it would have been valuable and people seemed to be less worried after they had realized it was very unlikely to be a problem and this is very this is very replicable I think if you have a developing situation worth considering something like Uh we built a tool for factecking individual claims. Uh and uh you know Steve's talk next uh we could maybe do something with that. Uh so anyway recap, it's a valuable problem. Currently AI notes only exist on X. If Elon has a bad day, they might not. That's that's bad. So we'd like instead they exist 100 times more than they currently do. So we'd like to live in this world, not this world. Big thanks to Jay and the X team and to FLF and uh also Yantalin previously funding our work here. Thanks very much.",
    "char_count": 8751,
    "success": true
  }
]